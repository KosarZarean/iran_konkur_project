"""
Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø´Ø§Ù…Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØªØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')


class ExamDataManager:
    """
    Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
    Ù…Ø³Ø¦ÙˆÙ„ÛŒØª: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒØŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    """
    
    def __init__(self, data_dir='data', recording_file='analysis.txt', plots_folder='plots'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯ÛŒØ± Ø¯Ø§Ø¯Ù‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        data_dir : str
            Ù¾ÙˆØ´Ù‡ Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        recording_file : str
            ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø«Ø¨Øª ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        plots_folder : str
            Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        """
        self.data_dir = data_dir
        self.recording_file = recording_file
        self.plots_folder = plots_folder
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(plots_folder, exist_ok=True)
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
        self.df = None
        self.X = None
        self.y = None
        self.num_classes = None
        self.target_col = None
        self.task_type = None
        self.feature_names = None
        
        # Ø¨Ø±Ø§ÛŒ TabTransformer
        self.categories = None
        self.continuous_features = 0
        self.X_cat = None
        self.X_cont = None
        
        # Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
        self.train_indices = None
        self.val_indices = None
        self.test_indices = None
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        
        # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        self.scaler = StandardScaler()
        self.minmax_scaler = MinMaxScaler()
        self.label_encoders = {}
        self.num_imputer = SimpleImputer(strategy='median')
        self.cat_imputer = SimpleImputer(strategy='most_frequent')
        
        print(f"ğŸ“ Ù…Ø¯ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: data_dir={data_dir}")
    
    def load_and_prepare_data(self, data_path=None, task_type='regression'):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        data_path : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡
        task_type : str
            Ù†ÙˆØ¹ ÙˆØ¸ÛŒÙÙ‡: 'regression' ÛŒØ§ 'classification'
        
        Returns:
        --------
        pd.DataFrame
            Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡
        """
        print("\n" + "="*60)
        print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
        print("="*60)
        
        self.task_type = task_type
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡
        if data_path is None:
            data_path = self._find_exam_data()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
        self._load_exam_data(data_path)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
        self._clean_data()
        
        # ØªØ¹Ø±ÛŒÙ ÙˆØ¸ÛŒÙÙ‡
        self._define_task()
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self._identify_columns()
        
        print("\n" + "="*60)
        print(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(self.df):,}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(self.df.columns)}")
        print("="*60)
        
        return self.df
    
    def _find_exam_data(self):
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        possible_paths = [
            'iran_exam.csv',
            'data/iran_exam.csv',
            '/content/iran_exam.csv',
            '../data/iran_exam.csv',
            './iran_exam.csv'
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ“ ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø³ÛŒØ± ÛŒØ§ÙØª Ø´Ø¯: {path}")
                return path
        
        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø³Ø§Ø²
        print("âš ï¸ ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        return self._create_sample_data()
    
    def _create_sample_data(self, n_samples=1000):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
        print("ğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡...")
        
        np.random.seed(42)
        data = {
            'Ø´Ù‡Ø±': np.random.choice(['ØªÙ‡Ø±Ø§Ù†', 'Ù…Ø´Ù‡Ø¯', 'Ø§ØµÙÙ‡Ø§Ù†', 'ØªØ¨Ø±ÛŒØ²', 'Ø´ÛŒØ±Ø§Ø²'], n_samples),
            'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ': np.random.randint(1, 200000, n_samples),
            'Ø±ØªØ¨Ù‡ Ø¯Ø± Ù…Ù†Ø·Ù‚Ù‡': np.random.randint(1, 50000, n_samples),
            'Ù…Ù†Ø·Ù‚Ù‡': np.random.choice(['Ù…Ù†Ø·Ù‚Ù‡1', 'Ù…Ù†Ø·Ù‚Ù‡2', 'Ù…Ù†Ø·Ù‚Ù‡3'], n_samples),
            'Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ±Ø§Ø² Ú©Ø§Ù†ÙˆÙ†': np.random.uniform(4000, 8000, n_samples),
            'ØªØ¹Ø¯Ø§Ø¯ Ø¢Ø²Ù…ÙˆÙ†': np.random.randint(1, 30, n_samples),
            'Ø±Ø´ØªÙ‡ Ù‚Ø¨ÙˆÙ„ÛŒ': np.random.choice(['Ù¾Ø²Ø´Ú©ÛŒ', 'Ù…Ù‡Ù†Ø¯Ø³ÛŒ', 'Ø­Ù‚ÙˆÙ‚'], n_samples),
            'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ù‚Ø¨ÙˆÙ„ÛŒ': np.random.choice(['ØªÙ‡Ø±Ø§Ù†', 'Ø´Ø±ÛŒÙ', 'Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±'], n_samples),
            'Ø³Ø§Ù„': np.random.choice([1398, 1399, 1400, 1401], n_samples)
        }
        
        df = pd.DataFrame(data)
        sample_path = 'data/sample_iran_exam.csv'
        os.makedirs('data', exist_ok=True)
        df.to_csv(sample_path, index=False)
        print(f"âœ… Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± {sample_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return sample_path
    
    def _load_exam_data(self, data_path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ CSV"""
        try:
            self.df = pd.read_csv(data_path)
            print(f"\nğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
            print(f"   Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {self.df.shape}")
            print(f"   Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(self.df.columns)}")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
            raise
    
    def _identify_columns(self):
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        self.num_cols = self.df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.cat_cols = self.df.select_dtypes(include=['object']).columns.tolist()
        
        if 'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ' in self.num_cols:
            self.num_cols.remove('Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ')
        
        print(f"\nğŸ“‹ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§:")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(self.num_cols)} - {self.num_cols[:5]}")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(self.cat_cols)} - {self.cat_cols[:5]}")
    
    def _clean_data(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        print("\nğŸ§¹ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        # 1. Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        self._handle_missing_values()
        
        # 2. Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self._standardize_categorical()
        
        # 3. Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        # self._remove_outliers()
        
        print("âœ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
    
    def _handle_missing_values(self):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡"""
        missing_before = self.df.isnull().sum().sum()
        
        if missing_before > 0:
            print(f"  ğŸ” Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ø´Ø¯: {missing_before}")
            
            for col in self.df.columns:
                if self.df[col].isnull().sum() > 0:
                    if col in self.num_cols:
                        # Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ù…ÛŒØ§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                        median_val = self.df[col].median()
                        self.df[col].fillna(median_val, inplace=True)
                        print(f"    {col}: Ù¾Ø± Ø´Ø¯Ù‡ Ø¨Ø§ Ù…ÛŒØ§Ù†Ù‡ ({median_val:.2f})")
                    else:
                        # Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                        mode_val = self.df[col].mode()[0] if len(self.df[col].mode()) > 0 else 'Ù†Ø§Ù…Ø´Ø®Øµ'
                        self.df[col].fillna(mode_val, inplace=True)
                        print(f"    {col}: Ù¾Ø± Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ø¯ ({mode_val})")
        
        missing_after = self.df.isnull().sum().sum()
        print(f"  âœ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {missing_after}")
    
    def _standardize_categorical(self):
        """Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        print("  ğŸ·ï¸ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ...")
        
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù†Ø·Ù‚Ù‡
        if 'Ù…Ù†Ø·Ù‚Ù‡' in self.df.columns:
            region_mapping = {
                'Ù…Ù†Ø·Ù‚Ù‡1': 'Ù…Ù†Ø·Ù‚Ù‡1', 'Ù…Ù†Ø·Ù‚Ù‡ÙŠÚ©': 'Ù…Ù†Ø·Ù‚Ù‡1', 'Ù…Ù†Ø·Ù‚Ù‡ÛŒÚ©': 'Ù…Ù†Ø·Ù‚Ù‡1', 'Ù…Ù†Ø·Ù‚Ù‡ 1': 'Ù…Ù†Ø·Ù‚Ù‡1',
                'Ù…Ù†Ø·Ù‚Ù‡2': 'Ù…Ù†Ø·Ù‚Ù‡2', 'Ù…Ù†Ø·Ù‚Ù‡Ø¯Ùˆ': 'Ù…Ù†Ø·Ù‚Ù‡2', 'Ù…Ù†Ø·Ù‚Ù‡ 2': 'Ù…Ù†Ø·Ù‚Ù‡2',
                'Ù…Ù†Ø·Ù‚Ù‡3': 'Ù…Ù†Ø·Ù‚Ù‡3', 'Ù…Ù†Ø·Ù‚Ù‡Ø³Ù‡': 'Ù…Ù†Ø·Ù‚Ù‡3', 'Ù…Ù†Ø·Ù‚Ù‡ 3': 'Ù…Ù†Ø·Ù‚Ù‡3'
            }
            
            before = self.df['Ù…Ù†Ø·Ù‚Ù‡'].nunique()
            self.df['Ù…Ù†Ø·Ù‚Ù‡'] = self.df['Ù…Ù†Ø·Ù‚Ù‡'].apply(
                lambda x: region_mapping.get(str(x).strip(), str(x).strip())
            )
            after = self.df['Ù…Ù†Ø·Ù‚Ù‡'].nunique()
            print(f"    Ù…Ù†Ø·Ù‚Ù‡: {before} â†’ {after} Ù…Ù‚Ø¯Ø§Ø± ÛŒÚ©ØªØ§")
        
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø´Ù‡Ø±Ù‡Ø§
        if 'Ø´Ù‡Ø±' in self.df.columns:
            self.df['Ø´Ù‡Ø±'] = self.df['Ø´Ù‡Ø±'].astype(str).str.strip().str.replace(r'\s+', ' ', regex=True)
            print(f"    Ø´Ù‡Ø±: {self.df['Ø´Ù‡Ø±'].nunique()} Ø´Ù‡Ø± ÛŒÚ©ØªØ§")
    
    def _remove_outliers(self, threshold=3):
        """Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª"""
        print("  âš ï¸ Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª...")
        
        for col in self.num_cols:
            if col != 'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ':
                Q1 = self.df[col].quantile(0.25)
                Q3 = self.df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                
                before = len(self.df)
                self.df = self.df[(self.df[col] >= lower_bound) & (self.df[col] <= upper_bound)]
                after = len(self.df)
                
                if before > after:
                    print(f"    {col}: {before - after} Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±Øª Ø­Ø°Ù Ø´Ø¯")
    
    def _define_task(self):
        """ØªØ¹Ø±ÛŒÙ ÙˆØ¸ÛŒÙÙ‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        print(f"\nğŸ¯ ØªØ¹Ø±ÛŒÙ ÙˆØ¸ÛŒÙÙ‡: {self.task_type}")
        
        if self.task_type == 'classification':
            # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ - 20% Ø¨Ø±ØªØ±
            if 'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ' in self.df.columns:
                threshold = self.df['Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ'].quantile(0.2)
                self.df['target'] = (self.df['Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ'] <= threshold).astype(int)
                self.target_col = 'target'
                
                class_counts = self.df['target'].value_counts()
                print(f"   Ø¢Ø³ØªØ§Ù†Ù‡: Ø±ØªØ¨Ù‡ â‰¤ {threshold:.0f}")
                print(f"   ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: Ú©Ù„Ø§Ø³ 0={class_counts.get(0, 0)} ({class_counts.get(0, 0)/len(self.df)*100:.1f}%), Ú©Ù„Ø§Ø³ 1={class_counts.get(1, 0)} ({class_counts.get(1, 0)/len(self.df)*100:.1f}%)")
        
        elif self.task_type == 'regression':
            # Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ØªØ¨Ù‡
            if 'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ' in self.df.columns:
                self.target_col = 'Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ'
                print(f"   Ù‡Ø¯Ù: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ")
                print(f"   Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø±ØªØ¨Ù‡: {self.df['Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ'].min():.0f} - {self.df['Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ'].max():.0f}")
        
        else:
            raise ValueError(f"Ù†ÙˆØ¹ ÙˆØ¸ÛŒÙÙ‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {self.task_type}")
    
    def prepare_for_traditional_models(self):
        """
        Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒ
        Ø´Ø§Ù…Ù„: Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        """
        print("\nğŸ”„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒ...")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ
        feature_cols = [col for col in self.df.columns if col != self.target_col]
        categorical_cols = [col for col in self.cat_cols if col in feature_cols]
        numerical_cols = [col for col in self.num_cols if col in feature_cols]
        
        X_list = []
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if numerical_cols:
            print(f"  ğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ({len(numerical_cols)}): {numerical_cols[:5]}")
            X_num = self.df[numerical_cols].values
            X_num = self.scaler.fit_transform(X_num)
            X_list.append(X_num)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        if categorical_cols:
            print(f"  ğŸ·ï¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ ({len(categorical_cols)}): {categorical_cols[:5]}")
            X_cat_list = []
            
            for col in categorical_cols:
                le = LabelEncoder()
                encoded = le.fit_transform(self.df[col].astype(str))
                X_cat_list.append(encoded.reshape(-1, 1))
                self.label_encoders[col] = le
            
            X_cat = np.hstack(X_cat_list)
            X_list.append(X_cat)
        
        # ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        if len(X_list) > 1:
            self.X = np.hstack(X_list)
        else:
            self.X = X_list[0]
        
        self.y = self.df[self.target_col].values
        self.feature_names = feature_cols
        
        print(f"  âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯:")
        print(f"     X shape: {self.X.shape}")
        print(f"     y shape: {self.y.shape}")
        
        return self.X, self.y
    
    def prepare_for_tabtransformer(self):
        """
        Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ TabTransformer
        Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø¹Ø¯Ø¯ÛŒ
        """
        print("\nğŸ”„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ TabTransformer...")
        
        categorical_cols = [col for col in self.cat_cols if col != self.target_col]
        numerical_cols = [col for col in self.num_cols if col != self.target_col]
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        if categorical_cols:
            X_cat_list = []
            self.categories = []
            
            for col in categorical_cols:
                le = LabelEncoder()
                encoded = le.fit_transform(self.df[col].astype(str))
                X_cat_list.append(encoded.reshape(-1, 1))
                self.categories.append(len(le.classes_))
                self.label_encoders[col] = le
            
            self.X_cat = np.hstack(X_cat_list).astype(np.int64)
            print(f"  ğŸ·ï¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {self.X_cat.shape}, categories: {self.categories}")
        else:
            self.X_cat = np.zeros((len(self.df), 0), dtype=np.int64)
            self.categories = []
            print(f"  ğŸ·ï¸ Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if numerical_cols:
            X_cont = self.df[numerical_cols].values.astype(np.float32)
            self.X_cont = self.scaler.fit_transform(X_cont)
            self.continuous_features = len(numerical_cols)
            print(f"  ğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {self.X_cont.shape}")
        else:
            self.X_cont = np.zeros((len(self.df), 0), dtype=np.float32)
            self.continuous_features = 0
            print(f"  ğŸ“Š Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ø¯Ø¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        self.y = self.df[self.target_col].values
        
        return self.X_cat, self.X_cont, self.y
    
    def create_train_val_test_split(self, train_size=0.7, val_size=0.15, test_size=0.15):
        """
        ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø³Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        train_size : float
            Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´
        val_size : float
            Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        test_size : float
            Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´
        """
        print(f"\nâœ‚ï¸ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: train={train_size}, val={val_size}, test={test_size}")
        
        # ØªÙ‚Ø³ÛŒÙ… Ø§ÙˆÙ„: Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ù…ÙˆÙ‚Øª
        X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(
            self.X, self.y, np.arange(len(self.X)),
            test_size=(val_size + test_size),
            random_state=42,
            stratify=self.y if self.task_type == 'classification' else None
        )
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¯ÙˆÙ…: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´
        val_ratio = val_size / (val_size + test_size)
        X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(
            X_temp, y_temp, idx_temp,
            test_size=test_size/(val_size+test_size),
            random_state=42,
            stratify=y_temp if self.task_type == 'classification' else None
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒØ³â€ŒÙ‡Ø§
        self.train_indices = idx_train
        self.val_indices = idx_val
        self.test_indices = idx_test
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.X_train, self.y_train = X_train, y_train
        self.X_val, self.y_val = X_val, y_val
        self.X_test, self.y_test = X_test, y_test
        
        print(f"  âœ… Ø¢Ù…ÙˆØ²Ø´: {len(X_train)} Ù†Ù…ÙˆÙ†Ù‡ ({len(X_train)/len(self.X)*100:.1f}%)")
        print(f"  âœ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(X_val)} Ù†Ù…ÙˆÙ†Ù‡ ({len(X_val)/len(self.X)*100:.1f}%)")
        print(f"  âœ… Ø¢Ø²Ù…Ø§ÛŒØ´: {len(X_test)} Ù†Ù…ÙˆÙ†Ù‡ ({len(X_test)/len(self.X)*100:.1f}%)")
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def exploratory_data_analysis(self):
        """
        ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        """
        print("\nğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        eda_dir = os.path.join(self.plots_folder, 'eda')
        os.makedirs(eda_dir, exist_ok=True)
        
        # 1. Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ
        self._basic_statistics()
        
        # 2. ØªØ­Ù„ÛŒÙ„ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù
        self._plot_target_distribution(eda_dir)
        
        # 3. ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        self._plot_numerical_features(eda_dir)
        
        # 4. ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self._plot_categorical_features(eda_dir)
        
        # 5. Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
        self._plot_correlation_matrix(eda_dir)
        
        print(f"âœ… ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¯Ø± {eda_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    
    def _basic_statistics(self):
        """Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ Ù¾Ø§ÛŒÙ‡"""
        print("\nğŸ“Š Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ:")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(self.df):,}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(self.df.columns)}")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(self.num_cols)}")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(self.cat_cols)}")
        print(f"   Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡: {self.df.isnull().sum().sum()}")
    
    def _plot_target_distribution(self, save_dir):
        """Ø±Ø³Ù… ØªÙˆØ²ÛŒØ¹ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù"""
        plt.figure(figsize=(12, 5))
        
        if self.task_type == 'classification':
            class_counts = self.df[self.target_col].value_counts()
            
            plt.subplot(1, 2, 1)
            plt.bar(class_counts.index, class_counts.values, color=['skyblue', 'lightcoral'])
            plt.title('ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù')
            plt.xlabel('Ú©Ù„Ø§Ø³')
            plt.ylabel('ØªØ¹Ø¯Ø§Ø¯')
            plt.xticks([0, 1], ['Ú©Ù„Ø§Ø³ 0', 'Ú©Ù„Ø§Ø³ 1'])
            
            plt.subplot(1, 2, 2)
            plt.pie(class_counts.values, labels=['Ú©Ù„Ø§Ø³ 0', 'Ú©Ù„Ø§Ø³ 1'], 
                   autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])
            plt.title('Ù†Ø³Ø¨Øª Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§')
            
        else:
            plt.subplot(1, 2, 1)
            plt.hist(self.df[self.target_col], bins=50, edgecolor='black', alpha=0.7)
            plt.title('ØªÙˆØ²ÛŒØ¹ Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ')
            plt.xlabel('Ø±ØªØ¨Ù‡')
            plt.ylabel('ØªØ¹Ø¯Ø§Ø¯')
            
            plt.subplot(1, 2, 2)
            plt.boxplot(self.df[self.target_col])
            plt.title('Boxplot Ø±ØªØ¨Ù‡ Ú©Ø´ÙˆØ±ÛŒ')
            plt.ylabel('Ø±ØªØ¨Ù‡')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'target_distribution.jpg'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_numerical_features(self, save_dir):
        """Ø±Ø³Ù… ØªÙˆØ²ÛŒØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ"""
        for col in self.num_cols[:6]:  # Ø­Ø¯Ø§Ú©Ø«Ø± Û¶ ÙˆÛŒÚ˜Ú¯ÛŒ
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.hist(self.df[col], bins=30, edgecolor='black', alpha=0.7)
            plt.title(f'ØªÙˆØ²ÛŒØ¹ {col}')
            plt.xlabel(col)
            plt.ylabel('ØªØ¹Ø¯Ø§Ø¯')
            
            plt.subplot(1, 2, 2)
            plt.boxplot(self.df[col])
            plt.title(f'Boxplot {col}')
            plt.ylabel(col)
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, f'dist_{col}.jpg'), dpi=300, bbox_inches='tight')
            plt.close()
    
    def _plot_categorical_features(self, save_dir):
        """Ø±Ø³Ù… ØªÙˆØ²ÛŒØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        for col in self.cat_cols[:3]:  # Ø­Ø¯Ø§Ú©Ø«Ø± Û³ ÙˆÛŒÚ˜Ú¯ÛŒ
            plt.figure(figsize=(12, 6))
            
            # Û±Ûµ Ø¯Ø³ØªÙ‡ Ø¨Ø±ØªØ±
            value_counts = self.df[col].value_counts().head(15)
            
            plt.bar(range(len(value_counts)), value_counts.values, color='skyblue', edgecolor='black')
            plt.title(f'ØªÙˆØ²ÛŒØ¹ {col} (Û±Ûµ Ø¯Ø³ØªÙ‡ Ø¨Ø±ØªØ±)')
            plt.xlabel(col)
            plt.ylabel('ØªØ¹Ø¯Ø§Ø¯')
            plt.xticks(range(len(value_counts)), value_counts.index, rotation=45, ha='right')
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, f'cat_{col}.jpg'), dpi=300, bbox_inches='tight')
            plt.close()
    
    def _plot_correlation_matrix(self, save_dir):
        """Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ"""
        if len(self.num_cols) >= 2:
            plt.figure(figsize=(12, 10))
            
            corr_matrix = self.df[self.num_cols].corr()
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            
            sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',
                       cmap='coolwarm', center=0, square=True,
                       linewidths=1, cbar_kws={"shrink": 0.8})
            
            plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ')
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, 'correlation_matrix.jpg'), dpi=300, bbox_inches='tight')
            plt.close()
    
    def get_data_summary(self):
        """Ú¯Ø±ÙØªÙ† Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡"""
        summary = {
            'total_samples': len(self.df),
            'total_features': len(self.df.columns),
            'numeric_features': len(self.num_cols),
            'categorical_features': len(self.cat_cols),
            'missing_values': self.df.isnull().sum().sum(),
            'task_type': self.task_type,
            'target_column': self.target_col
        }
        
        if self.task_type == 'classification':
            summary['class_distribution'] = self.df[self.target_col].value_counts().to_dict()
        else:
            summary['target_min'] = self.df[self.target_col].min()
            summary['target_max'] = self.df[self.target_col].max()
            summary['target_mean'] = self.df[self.target_col].mean()
        
        return summary


# Ú©Ù„Ø§Ø³ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ø±ÛŒØ¹
class ExamDataAnalyzer:
    """
    Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø³Ø±ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    """
    
    def __init__(self, df):
        self.df = df
        self.num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.cat_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    def quick_summary(self):
        """Ú¯Ø²Ø§Ø±Ø´ Ø³Ø±ÛŒØ¹"""
        print("\nğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø³Ø±ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(self.df):,}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(self.df.columns)}")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(self.num_cols)}")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(self.cat_cols)}")
        print(f"   Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡: {self.df.isnull().sum().sum()}")
        
        if len(self.num_cols) > 0:
            print("\n   Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ:")
            print(self.df[self.num_cols].describe().round(2))
    
    def check_missing(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡"""
        missing = self.df.isnull().sum()
        missing = missing[missing > 0]
        
        if len(missing) > 0:
            print("\nğŸ” Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡:")
            for col, val in missing.items():
                print(f"   {col}: {val} ({val/len(self.df)*100:.1f}%)")
        else:
            print("\nâœ… Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        return missing
    
    def check_duplicates(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        duplicates = self.df.duplicated().sum()
        if duplicates > 0:
            print(f"\nâš ï¸ {duplicates} Ø¯Ø§Ø¯Ù‡ ØªÚ©Ø±Ø§Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")
        else:
            print("\nâœ… Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ ØªÚ©Ø±Ø§Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        return duplicates
