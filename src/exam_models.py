"""
Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ PyTorch Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø´Ø§Ù…Ù„: MLPØŒ TabTransformerØŒ Regressor Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Dataset
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset
import math


# ============================================
# Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Dataset
# ============================================

class ExamDataset(Dataset):
    """
    Dataset Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (MLP, Regressor)
    """
    def __init__(self, X, y):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X : array-like
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        y : array-like
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
        """
        self.X = torch.FloatTensor(X)
        
        if len(y.shape) == 1:
            self.y = torch.FloatTensor(y)
        else:
            self.y = torch.LongTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class TabTransformerDataset(Dataset):
    """
    Dataset Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ TabTransformer
    """
    def __init__(self, X_cat, X_cont, y):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X_cat : array-like
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        X_cont : array-like
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        y : array-like
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
        """
        self.X_cat = torch.LongTensor(X_cat)
        self.X_cont = torch.FloatTensor(X_cont)
        
        if len(y.shape) == 1:
            self.y = torch.FloatTensor(y)
        else:
            self.y = torch.LongTensor(y)
    
    def __len__(self):
        return len(self.X_cat)
    
    def __getitem__(self, idx):
        return self.X_cat[idx], self.X_cont[idx], self.y[idx]


# ============================================
# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
# ============================================

class ExamMLP(nn.Module):
    """
    Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ (MLP) Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
    """
    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=1, 
                 dropout=0.2, activation='relu', use_batch_norm=True):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        input_dim : int
            Ø¨Ø¹Ø¯ ÙˆØ±ÙˆØ¯ÛŒ
        hidden_dims : list
            Ø§Ø¨Ø¹Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†
        output_dim : int
            Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ (1 Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†ØŒ >1 Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)
        dropout : float
            Ù†Ø±Ø® Dropout
        activation : str
            ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø² ('relu', 'leaky_relu', 'elu', 'tanh')
        use_batch_norm : bool
            Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Batch Normalization
        """
        super(ExamMLP, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.dropout_rate = dropout
        self.use_batch_norm = use_batch_norm
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU(0.1)
        elif activation == 'elu':
            self.activation = nn.ELU()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            self.activation = nn.ReLU()
        
        # Ø³Ø§Ø®Øª Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            # Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ
            layers.append(nn.Linear(prev_dim, hidden_dim))
            
            # Batch Normalization
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            
            # ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²
            layers.append(self.activation)
            
            # Dropout
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            
            prev_dim = hidden_dim
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.model = nn.Sequential(*layers)
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§
        self._init_weights()
    
    def _init_weights(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ø±ÙˆØ´ Kaiming"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø´Ø¨Ú©Ù‡"""
        return self.model(x)
    
    def get_feature_importance(self, feature_names=None):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        feature_names : list
            Ù†Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        
        Returns:
        --------
        dict
            Ø§Ù‡Ù…ÛŒØª Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ
        """
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„
        first_layer = self.model[0]
        weights = first_layer.weight.data.cpu().numpy()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‚Ø¯Ø± Ù…Ø·Ù„Ù‚ ÙˆØ²Ù†â€ŒÙ‡Ø§)
        importance = np.mean(np.abs(weights), axis=0)
        
        if feature_names is not None:
            if len(feature_names) != len(importance):
                print(f"âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ({len(feature_names)}) Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ({len(importance)}) Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯")
                return dict(enumerate(importance))
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª
            sorted_idx = np.argsort(importance)[::-1]
            return {feature_names[i]: importance[i] for i in sorted_idx}
        else:
            return dict(enumerate(importance))


class ExamRegressor(nn.Module):
    """
    Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ØªØ¨Ù‡
    """
    def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.2):
        super(ExamRegressor, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x).squeeze()


# ============================================
# Ù…Ø¯Ù„ TabTransformer
# ============================================

class TransformerBlock(nn.Module):
    """
    ÛŒÚ© Ø¨Ù„ÙˆÚ© Transformer Ø´Ø§Ù…Ù„ Self-Attention Ùˆ Feed-Forward
    """
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):
        super(TransformerBlock, self).__init__()
        
        # Self-Attention
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        
        # Feed-Forward
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        self.activation = nn.ReLU()
    
    def forward(self, x):
        # Self-Attention with residual connection
        attn_output, _ = self.self_attn(x, x, x)
        x = x + self.dropout1(attn_output)
        x = self.norm1(x)
        
        # Feed-Forward with residual connection
        ff_output = self.linear2(self.dropout(self.activation(self.linear1(x))))
        x = x + self.dropout2(ff_output)
        x = self.norm2(x)
        
        return x


class TabTransformer(nn.Module):
    """
    Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ TabTransformer
    Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø§Ù„Ù‡: TabTransformer: Tabular Data Modeling Using Contextual Embeddings
    """
    def __init__(self, num_categorical, num_continuous, categories,
                 embedding_dim=32, num_heads=4, num_layers=3,
                 mlp_hidden_dims=[128, 64], mlp_dropout=0.2,
                 transformer_dropout=0.1, output_dim=1):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        num_categorical : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        num_continuous : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        categories : list
            Ù„ÛŒØ³Øª ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        num_heads : int
            ØªØ¹Ø¯Ø§Ø¯ headÙ‡Ø§ÛŒ attention
        num_layers : int
            ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        mlp_hidden_dims : list
            Ø§Ø¨Ø¹Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† MLP Ù†Ù‡Ø§ÛŒÛŒ
        mlp_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± MLP
        transformer_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± Transformer
        output_dim : int
            Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ (1 Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†)
        """
        super(TabTransformer, self).__init__()
        
        self.num_categorical = num_categorical
        self.num_continuous = num_continuous
        self.categories = categories
        self.embedding_dim = embedding_dim
        
        # 1. Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Embedding Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.cat_embeddings = nn.ModuleList([
            nn.Embedding(cat, embedding_dim) for cat in categories
        ])
        
        # 2. Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(embedding_dim, num_heads, 
                           dim_feedforward=embedding_dim*4, 
                           dropout=transformer_dropout)
            for _ in range(num_layers)
        ])
        
        # 3. Ù„Ø§ÛŒÙ‡ Projection Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if num_continuous > 0:
            self.cont_projection = nn.Sequential(
                nn.Linear(num_continuous, embedding_dim),
                nn.LayerNorm(embedding_dim),
                nn.ReLU(),
                nn.Dropout(mlp_dropout)
            )
        
        # 4. MLP Ù†Ù‡Ø§ÛŒÛŒ
        mlp_input_dim = embedding_dim * num_categorical + (embedding_dim if num_continuous > 0 else 0)
        
        mlp_layers = []
        prev_dim = mlp_input_dim
        
        for hidden_dim in mlp_hidden_dims:
            mlp_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(mlp_dropout)
            ])
            prev_dim = hidden_dim
        
        mlp_layers.append(nn.Linear(prev_dim, output_dim))
        
        self.mlp = nn.Sequential(*mlp_layers)
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._init_weights()
    
    def _init_weights(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x_cat, x_cont):
        """
        Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x_cat : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_categorical)
        x_cont : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_continuous)
        
        Returns:
        --------
        torch.Tensor
            Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„
        """
        batch_size = x_cat.shape[0]
        
        # 1. Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        cat_embedded = []
        for i in range(self.num_categorical):
            emb = self.cat_embeddings[i](x_cat[:, i])
            cat_embedded.append(emb)
        
        # Ø´Ú©Ù„: (batch_size, num_categorical, embedding_dim)
        cat_embedded = torch.stack(cat_embedded, dim=1)
        
        # 2. Ø¹Ø¨ÙˆØ± Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        transformer_out = cat_embedded
        for transformer in self.transformer_blocks:
            transformer_out = transformer(transformer_out)
        
        # 3. Flatten Ú©Ø±Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ Transformer
        transformer_flat = transformer_out.reshape(batch_size, -1)
        
        # 4. Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if self.num_continuous > 0:
            cont_embedded = self.cont_projection(x_cont)
            # ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ Ø®Ø±ÙˆØ¬ÛŒ Transformer
            combined = torch.cat([transformer_flat, cont_embedded], dim=1)
        else:
            combined = transformer_flat
        
        # 5. MLP Ù†Ù‡Ø§ÛŒÛŒ
        output = self.mlp(combined)
        
        return output.squeeze() if output.shape[1] == 1 else output
    
    def get_attention_weights(self, x_cat):
        """
        Ø¯Ø±ÛŒØ§ÙØª ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention Ø¨Ø±Ø§ÛŒ ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x_cat : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        
        Returns:
        --------
        list
            ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention Ù‡Ø± Ù„Ø§ÛŒÙ‡
        """
        self.eval()
        attention_weights = []
        
        with torch.no_grad():
            # Embedding
            cat_embedded = []
            for i in range(self.num_categorical):
                emb = self.cat_embeddings[i](x_cat[:, i])
                cat_embedded.append(emb)
            
            cat_embedded = torch.stack(cat_embedded, dim=1)
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention Ø§Ø² Ù‡Ø± Ù„Ø§ÛŒÙ‡
            x = cat_embedded
            for transformer in self.transformer_blocks:
                # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)
                if hasattr(transformer.self_attn, 'get_attention_weights'):
                    attn_weights = transformer.self_attn.get_attention_weights(x)
                    attention_weights.append(attn_weights)
                
                x = transformer(x)
        
        return attention_weights


# ============================================
# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ
# ============================================

class EarlyStopping:
    """
    Early stopping Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting
    """
    def __init__(self, patience=10, min_delta=0.001, verbose=True, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.restore_best_weights = restore_best_weights
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_model_state = None
        self.best_epoch = 0
    
    def __call__(self, score, model, epoch):
        """
        Ø¨Ø±Ø±Ø³ÛŒ early stopping
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        score : float
            Ø§Ù…ØªÛŒØ§Ø² validation (Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª)
        model : nn.Module
            Ù…Ø¯Ù„
        epoch : int
            Ø´Ù…Ø§Ø±Ù‡ epoch
        """
        if self.best_score is None:
            self.best_score = score
            self.best_model_state = model.state_dict().copy()
            self.best_epoch = epoch
            if self.verbose:
                print(f"      ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ø§ÙˆÙ„ÛŒÙ‡: {self.best_score:.4f}")
        
        elif score - self.best_score > self.min_delta:
            self.best_score = score
            self.best_model_state = model.state_dict().copy()
            self.best_epoch = epoch
            self.counter = 0
            if self.verbose:
                print(f"      ğŸ“ˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡: {self.best_score:.4f}")
        
        else:
            self.counter += 1
            if self.verbose and self.counter % 5 == 0:
                print(f"      â³ Ø¹Ø¯Ù… Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {self.counter}/{self.patience} epoch")
            
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"      ğŸ›‘ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø± epoch {epoch}")
                
                if self.restore_best_weights and self.best_model_state is not None:
                    model.load_state_dict(self.best_model_state)
                    if self.verbose:
                        print(f"      ğŸ”„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø² epoch {self.best_epoch}")
    
    def reset(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ"""
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_model_state = None
        self.best_epoch = 0


class ModelUtils:
    """
    ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
    """
    
    @staticmethod
    def count_parameters(model):
        """
        Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        model : nn.Module
            Ù…Ø¯Ù„
        
        Returns:
        --------
        dict
            ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        """
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total': total_params,
            'trainable': trainable_params,
            'non_trainable': total_params - trainable_params
        }
    
    @staticmethod
    def get_device():
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§
        
        Returns:
        --------
        torch.device
            Ø¯Ø³ØªÚ¯Ø§Ù‡ (cuda/cpu)
        """
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"âœ… CUDA available: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device('cpu')
            print("âš ï¸ CUDA not available, using CPU")
        
        return device


# ============================================
# ØªØ§Ø¨Ø¹ ØªØ³Øª
# ============================================

def test_models():
    """ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡"""
    print("ğŸ§ª ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ PyTorch")
    print("="*60)
    
    device = ModelUtils.get_device()
    
    # Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    batch_size = 32
    n_cat = 3
    n_cont = 5
    categories = [10, 5, 8]  # 10, 5, 8 Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
    
    x_cat = torch.randint(0, 10, (batch_size, n_cat))
    x_cont = torch.randn(batch_size, n_cont)
    
    # 1. ØªØ³Øª MLP
    print("\n1ï¸âƒ£ ØªØ³Øª ExamMLP:")
    mlp = ExamMLP(input_dim=10, hidden_dims=[64, 32], output_dim=1)
    mlp.to(device)
    output = mlp(x_cont.to(device))
    params = ModelUtils.count_parameters(mlp)
    print(f"   Ø®Ø±ÙˆØ¬ÛŒ shape: {output.shape}")
    print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {params['total']:,}")
    
    # 2. ØªØ³Øª TabTransformer
    print("\n2ï¸âƒ£ ØªØ³Øª TabTransformer:")
    tab = TabTransformer(
        num_categorical=n_cat,
        num_continuous=n_cont,
        categories=categories,
        embedding_dim=32,
        num_heads=4,
        num_layers=3
    )
    tab.to(device)
    output = tab(x_cat.to(device), x_cont.to(device))
    params = ModelUtils.count_parameters(tab)
    print(f"   Ø®Ø±ÙˆØ¬ÛŒ shape: {output.shape}")
    print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {params['total']:,}")
    
    # 3. ØªØ³Øª EarlyStopping
    print("\n3ï¸âƒ£ ØªØ³Øª EarlyStopping:")
    early_stopping = EarlyStopping(patience=5, verbose=True)
    
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
    scores = [0.8, 0.82, 0.81, 0.83, 0.82, 0.82, 0.81, 0.80]
    for i, score in enumerate(scores):
        early_stopping(score, mlp, i)
        if early_stopping.early_stop:
            break
    
    print("\nâœ… Ù‡Ù…Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")


if __name__ == "__main__":
    test_models()
