"""
Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ PyTorch Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø´Ø§Ù…Ù„: ØªÙˆØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

from exam_models import ExamDataset, TabTransformerDataset, EarlyStopping, ModelUtils


class ExamTrainer:
    """
    Ú©Ù„Ø§Ø³ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ PyTorch
    Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² MLPØŒ TabTransformer Ùˆ Regressor
    """
    
    def __init__(self, model, model_type='mlp', device=None, 
                 model_name='model', save_dir='models'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ trainer
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        model : nn.Module
            Ù…Ø¯Ù„ PyTorch
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„: 'mlp', 'tabtransformer', 'regressor'
        device : torch.device
            Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø§Ø¬Ø±Ø§
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡
        save_dir : str
            Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        """
        self.model = model
        self.model_type = model_type
        self.model_name = model_name
        self.save_dir = save_dir
        
        # ØªØ¹ÛŒÛŒÙ† Ø¯Ø³ØªÚ¯Ø§Ù‡
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        
        self.model.to(self.device)
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¢Ù…ÙˆØ²Ø´
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_rmse': [],
            'val_rmse': [],
            'train_mae': [],
            'val_mae': [],
            'train_r2': [],
            'val_r2': [],
            'learning_rate': [],
            'epoch_time': []
        }
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        self.best_model_state = None
        self.best_epoch = 0
        self.best_val_rmse = float('inf')
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"âœ… Trainer Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯:")
        print(f"   Ù…Ø¯Ù„: {model_type}")
        print(f"   Ø¯Ø³ØªÚ¯Ø§Ù‡: {self.device}")
    
    def create_dataloaders(self, 
                          X_cat_train=None, X_cont_train=None, y_train=None,
                          X_cat_val=None, X_cont_val=None, y_val=None,
                          X_train=None, y_train_mlp=None, 
                          X_val=None, y_val_mlp=None,
                          batch_size=64, num_workers=2):
        """
        Ø§ÛŒØ¬Ø§Ø¯ DataLoader Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X_cat_train, X_cont_train, y_train : array
            Ø¯Ø§Ø¯Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer (Ø¢Ù…ÙˆØ²Ø´)
        X_cat_val, X_cont_val, y_val : array
            Ø¯Ø§Ø¯Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer (Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ)
        X_train, y_train_mlp : array
            Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ MLP (Ø¢Ù…ÙˆØ²Ø´)
        X_val, y_val_mlp : array
            Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ MLP (Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ)
        batch_size : int
            Ø§Ù†Ø¯Ø§Ø²Ù‡ batch
        num_workers : int
            ØªØ¹Ø¯Ø§Ø¯ workers Ø¨Ø±Ø§ÛŒ DataLoader
        """
        print("\nğŸ“¦ Ø§ÛŒØ¬Ø§Ø¯ DataLoader...")
        
        if self.model_type == 'tabtransformer':
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ TabTransformer
            if X_cat_train is None or X_cont_train is None or y_train is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ TabTransformer Ø¨Ø§ÛŒØ¯ X_cat_train, X_cont_train Ùˆ y_train Ù…Ø´Ø®Øµ Ø´ÙˆÙ†Ø¯")
            
            # Dataset Ø¨Ø±Ø§ÛŒ TabTransformer
            train_dataset = TabTransformerDataset(
                X_cat_train, X_cont_train, y_train
            )
            val_dataset = TabTransformerDataset(
                X_cat_val, X_cont_val, y_val
            )
            print(f"   ğŸ“Š TabTransformer: categorical={X_cat_train.shape[1]}, continuous={X_cont_train.shape[1]}")
            
        else:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ MLP
            if X_train is None or y_train_mlp is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ MLP Ø¨Ø§ÛŒØ¯ X_train Ùˆ y_train_mlp Ù…Ø´Ø®Øµ Ø´ÙˆÙ†Ø¯")
            
            # Dataset Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
            train_dataset = ExamDataset(X_train, y_train_mlp)
            val_dataset = ExamDataset(X_val, y_val_mlp)
            print(f"   ğŸ“Š MLP: features={X_train.shape[1]}")
        
        # DataLoader
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        self.val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"   âœ… Train: {len(train_dataset)} Ù†Ù…ÙˆÙ†Ù‡ ({len(self.train_loader)} batch)")
        print(f"   âœ… Val: {len(val_dataset)} Ù†Ù…ÙˆÙ†Ù‡ ({len(self.val_loader)} batch)")
    
    def train_epoch(self):
        """ÛŒÚ© Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´"""
        self.model.train()
        total_loss = 0
        all_preds = []
        all_targets = []
        
        for batch in self.train_loader:
            if self.model_type == 'tabtransformer':
                x_cat, x_cont, y = batch
                x_cat = x_cat.to(self.device)
                x_cont = x_cont.to(self.device)
                y = y.to(self.device)
                
                # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
                self.optimizer.zero_grad()
                output = self.model(x_cat, x_cont)
                
            else:
                x, y = batch
                x = x.to(self.device)
                y = y.to(self.device)
                
                # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
                self.optimizer.zero_grad()
                output = self.model(x)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
            loss = self.criterion(output, y)
            
            # Ù¾Ø³â€ŒØ±Ùˆ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            loss.backward()
            
            # Gradient clipping (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            if hasattr(self, 'clip_grad_norm') and self.clip_grad_norm:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)
            
            self.optimizer.step()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
            total_loss += loss.item()
            all_preds.extend(output.detach().cpu().numpy())
            all_targets.extend(y.cpu().numpy())
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        avg_loss = total_loss / len(self.train_loader)
        y_pred = np.array(all_preds)
        y_true = np.array(all_targets)
        
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        return avg_loss, rmse, mae, r2
    
    def validate_epoch(self):
        """ÛŒÚ© Ø¯ÙˆØ±Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                if self.model_type == 'tabtransformer':
                    x_cat, x_cont, y = batch
                    x_cat = x_cat.to(self.device)
                    x_cont = x_cont.to(self.device)
                    y = y.to(self.device)
                    
                    output = self.model(x_cat, x_cont)
                    
                else:
                    x, y = batch
                    x = x.to(self.device)
                    y = y.to(self.device)
                    
                    output = self.model(x)
                
                loss = self.criterion(output, y)
                
                total_loss += loss.item()
                all_preds.extend(output.cpu().numpy())
                all_targets.extend(y.cpu().numpy())
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        avg_loss = total_loss / len(self.val_loader)
        y_pred = np.array(all_preds)
        y_true = np.array(all_targets)
        
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        return avg_loss, rmse, mae, r2
    
    def train(self, epochs=100, lr=0.001, weight_decay=1e-5, task_type='regression',
             patience=15, min_delta=0.001, clip_grad_norm=None,
             scheduler=None, verbose=True):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        epochs : int
            ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§
        lr : float
            Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        weight_decay : float
            ØªÙ†Ø¸ÛŒÙ… L2
        task_type : str
            Ù†ÙˆØ¹ ÙˆØ¸ÛŒÙÙ‡ ('regression' ÛŒØ§ 'classification')
        patience : int
            ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù…Ù„ Ø¨Ø±Ø§ÛŒ early stopping
        min_delta : float
            Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ù‡Ø¨ÙˆØ¯
        clip_grad_norm : float
            Ø­Ø¯Ø§Ú©Ø«Ø± Ù†Ø±Ù… Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
        scheduler : torch.optim.lr_scheduler
            Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ² Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        verbose : bool
            Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª
        """
        print("\n" + "="*80)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„")
        print("="*80)
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.clip_grad_norm = clip_grad_norm
        self.criterion = nn.MSELoss()  # Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
        
        # Early stopping
        from exam_models import EarlyStopping
        early_stopping = EarlyStopping(
            patience=patience,
            min_delta=min_delta,
            verbose=verbose
        )
        
        # Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹
        start_time = time.time()
        
        for epoch in range(1, epochs + 1):
            epoch_start = time.time()
            
            # Ø¢Ù…ÙˆØ²Ø´
            train_loss, train_rmse, train_mae, train_r2 = self.train_epoch()
            
            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
            val_loss, val_rmse, val_mae, val_r2 = self.validate_epoch()
            
            # Ø²Ù…Ø§Ù† Ø¯ÙˆØ±Ù‡
            epoch_time = time.time() - epoch_start
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_rmse'].append(train_rmse)
            self.history['val_rmse'].append(val_rmse)
            self.history['train_mae'].append(train_mae)
            self.history['val_mae'].append(val_mae)
            self.history['train_r2'].append(train_r2)
            self.history['val_r2'].append(val_r2)
            self.history['epoch_time'].append(epoch_time)
            self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ scheduler
            if scheduler is not None:
                scheduler.step(val_loss)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
            if val_rmse < self.best_val_rmse:
                self.best_val_rmse = val_rmse
                self.best_epoch = epoch
                self.best_model_state = self.model.state_dict().copy()
            
            # Early stopping
            early_stopping(-val_rmse, self.model, epoch)
            
            # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
            if verbose and (epoch % 10 == 0 or epoch == 1 or early_stopping.early_stop):
                print(f"\nğŸ“Š Epoch {epoch}/{epochs}")
                print(f"   Train - Loss: {train_loss:.4f}, RMSE: {train_rmse:.2f}, RÂ²: {train_r2:.4f}")
                print(f"   Val   - Loss: {val_loss:.4f}, RMSE: {val_rmse:.2f}, RÂ²: {val_r2:.4f}")
                print(f"   Ø²Ù…Ø§Ù†: {epoch_time:.2f}s, LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            if early_stopping.early_stop:
                print(f"\nğŸ›‘ Early stopping Ø¯Ø± epoch {epoch}")
                break
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            print(f"\nâœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø² epoch {self.best_epoch} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            print(f"   Ø¨Ù‡ØªØ±ÛŒÙ† RMSE Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {self.best_val_rmse:.2f}")
        
        # Ø²Ù…Ø§Ù† Ú©Ù„
        total_time = time.time() - start_time
        print(f"\nâ±ï¸  Ø²Ù…Ø§Ù† Ú©Ù„ Ø¢Ù…ÙˆØ²Ø´: {total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡ ({total_time/60:.2f} Ø¯Ù‚ÛŒÙ‚Ù‡)")
        print("="*80)
    
    def evaluate(self, 
                X_cat_test=None, X_cont_test=None, y_test=None,
                X_test=None, y_test_mlp=None,
                batch_size=64):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X_cat_test, X_cont_test, y_test : array
            Ø¯Ø§Ø¯Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer
        X_test, y_test_mlp : array
            Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ MLP
        batch_size : int
            Ø§Ù†Ø¯Ø§Ø²Ù‡ batch
        
        Returns:
        --------
        dict
            Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        """
        print("\nğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´...")
        
        self.model.eval()
        
        # Ø§ÛŒØ¬Ø§Ø¯ DataLoader
        if self.model_type == 'tabtransformer':
            if X_cat_test is None or X_cont_test is None or y_test is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ TabTransformer Ø¨Ø§ÛŒØ¯ X_cat_test, X_cont_test Ùˆ y_test Ù…Ø´Ø®Øµ Ø´ÙˆÙ†Ø¯")
            
            test_dataset = TabTransformerDataset(X_cat_test, X_cont_test, y_test)
        else:
            if X_test is None or y_test_mlp is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ MLP Ø¨Ø§ÛŒØ¯ X_test Ùˆ y_test_mlp Ù…Ø´Ø®Øµ Ø´ÙˆÙ†Ø¯")
            
            test_dataset = ExamDataset(X_test, y_test_mlp)
        
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in test_loader:
                if self.model_type == 'tabtransformer':
                    x_cat, x_cont, y = batch
                    x_cat = x_cat.to(self.device)
                    x_cont = x_cont.to(self.device)
                    output = self.model(x_cat, x_cont)
                    
                else:
                    x, y = batch
                    x = x.to(self.device)
                    output = self.model(x)
                
                all_preds.extend(output.cpu().numpy())
                all_targets.extend(y.numpy())
        
        y_pred = np.array(all_preds)
        y_true = np.array(all_targets)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        results = {
            'y_true': y_true,
            'y_pred': y_pred,
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred)
        }
        
        print(f"\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
        print(f"   RMSE: {results['rmse']:.2f}")
        print(f"   MAE: {results['mae']:.2f}")
        print(f"   RÂ²: {results['r2']:.4f}")
        
        return results
    
    def predict(self, 
               X_cat=None, X_cont=None,
               X=None,
               batch_size=64):
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X_cat, X_cont : array
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer
        X : array
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ MLP
        batch_size : int
            Ø§Ù†Ø¯Ø§Ø²Ù‡ batch
        
        Returns:
        --------
        array
            Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
        """
        self.model.eval()
        
        if self.model_type == 'tabtransformer':
            if X_cat is None or X_cont is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ TabTransformer Ø¨Ø§ÛŒØ¯ X_cat Ùˆ X_cont Ù…Ø´Ø®Øµ Ø´ÙˆÙ†Ø¯")
            
            dataset = TabTransformerDataset(X_cat, X_cont, np.zeros(len(X_cat)))
        else:
            if X is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ MLP Ø¨Ø§ÛŒØ¯ X Ù…Ø´Ø®Øµ Ø´ÙˆØ¯")
            
            dataset = ExamDataset(X, np.zeros(len(X)))
        
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        
        predictions = []
        
        with torch.no_grad():
            for batch in loader:
                if self.model_type == 'tabtransformer':
                    x_cat, x_cont, _ = batch
                    x_cat = x_cat.to(self.device)
                    x_cont = x_cont.to(self.device)
                    output = self.model(x_cat, x_cont)
                    
                else:
                    x, _ = batch
                    x = x.to(self.device)
                    output = self.model(x)
                
                predictions.extend(output.cpu().numpy())
        
        return np.array(predictions)
    
    def plot_history(self, save_path='plots/training_history.jpg'):
        """
        Ø±Ø³Ù… ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¢Ù…ÙˆØ²Ø´
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        save_path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±
        """
        epochs = range(1, len(self.history['train_loss']) + 1)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. Loss
        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSE
        axes[0, 1].plot(epochs, self.history['train_rmse'], 'b-', label='Train RMSE', linewidth=2)
        axes[0, 1].plot(epochs, self.history['val_rmse'], 'r-', label='Val RMSE', linewidth=2)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].set_title('Training and Validation RMSE')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. MAE
        axes[0, 2].plot(epochs, self.history['train_mae'], 'b-', label='Train MAE', linewidth=2)
        axes[0, 2].plot(epochs, self.history['val_mae'], 'r-', label='Val MAE', linewidth=2)
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('MAE')
        axes[0, 2].set_title('Training and Validation MAE')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. RÂ²
        axes[1, 0].plot(epochs, self.history['train_r2'], 'b-', label='Train RÂ²', linewidth=2)
        axes[1, 0].plot(epochs, self.history['val_r2'], 'r-', label='Val RÂ²', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('RÂ²')
        axes[1, 0].set_title('Training and Validation RÂ²')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Learning Rate
        axes[1, 1].plot(epochs, self.history['learning_rate'], 'g-', linewidth=2)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_title('Learning Rate Schedule')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Epoch Time
        axes[1, 2].plot(epochs, self.history['epoch_time'], 'm-', linewidth=2)
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('Time (seconds)')
        axes[1, 2].set_title('Epoch Training Time')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.suptitle(f'Training History - {self.model_name}', fontsize=16, y=1.02)
        plt.tight_layout()
        
        # Ø°Ø®ÛŒØ±Ù‡
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def save_model(self, filename=None):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filename : str
            Ù†Ø§Ù… ÙØ§ÛŒÙ„
        """
        if filename is None:
            filename = f'{self.model_name}_best.pt'
        
        save_path = os.path.join(self.save_dir, filename)
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'best_model_state': self.best_model_state,
            'best_val_rmse': self.best_val_rmse,
            'best_epoch': self.best_epoch,
            'history': self.history,
            'model_type': self.model_type,
            'model_name': self.model_name
        }
        
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return save_path
    
    def load_model(self, path):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        path : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…Ø¯Ù„
        """
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.best_model_state = checkpoint.get('best_model_state')
        self.best_val_rmse = checkpoint.get('best_val_rmse', float('inf'))
        self.best_epoch = checkpoint.get('best_epoch', 0)
        self.history = checkpoint.get('history', self.history)
        
        print(f"ğŸ“‚ Ù…Ø¯Ù„ Ø§Ø² {path} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        print(f"   Ø¨Ù‡ØªØ±ÛŒÙ† RMSE: {self.best_val_rmse:.2f} (epoch {self.best_epoch})")


# ============================================
# ØªØ§Ø¨Ø¹ ØªØ³Øª
# ============================================

def test_trainer():
    """ØªØ³Øª trainer Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡"""
    print("ğŸ§ª ØªØ³Øª ExamTrainer")
    print("="*60)
    
    from exam_models import ExamMLP
    
    # Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    X_train = np.random.randn(1000, 10)
    y_train = np.random.randn(1000)
    X_val = np.random.randn(200, 10)
    y_val = np.random.randn(200)
    X_test = np.random.randn(200, 10)
    y_test = np.random.randn(200)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
    model = ExamMLP(input_dim=10, hidden_dims=[64, 32], output_dim=1)
    
    # Ø§ÛŒØ¬Ø§Ø¯ trainer
    trainer = ExamTrainer(model, model_type='mlp', model_name='test_model')
    
    # Ø§ÛŒØ¬Ø§Ø¯ dataloader
    trainer.create_dataloaders(
        X_train=X_train, 
        y_train_mlp=y_train,
        X_val=X_val, 
        y_val_mlp=y_val,
        batch_size=32
    )
    
    # Ø¢Ù…ÙˆØ²Ø´
    trainer.train(epochs=10, verbose=True)
    
    # Ø±Ø³Ù… ØªØ§Ø±ÛŒØ®Ú†Ù‡
    trainer.plot_history('plots/test_history.jpg')
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    results = trainer.evaluate(X_test=X_test, y_test_mlp=y_test)
    
    print("\nâœ… ØªØ³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
    return results


if __name__ == "__main__":
    test_trainer()
