"""
ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø´Ø§Ù…Ù„: Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒØŒ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ØŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ Ùˆ ...
"""

import os
import json
import pickle
import joblib
import numpy as np
import pandas as pd
import torch
from datetime import datetime
from pathlib import Path
import hashlib
import random


class ExamUtils:
    """
    Ú©Ù„Ø§Ø³ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
    """
    
    @staticmethod
    def set_seed(seed=42):
        """
        ØªÙ†Ø¸ÛŒÙ… seed Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        seed : int
            Ù…Ù‚Ø¯Ø§Ø± seed
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        
        print(f"ğŸ² Seed ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯: {seed}")
    
    @staticmethod
    def ensure_dir(directory):
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        directory : str
            Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡
        
        Returns:
        --------
        str
            Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡
        """
        Path(directory).mkdir(parents=True, exist_ok=True)
        return directory
    
    @staticmethod
    def get_timestamp():
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø²Ù…Ø§Ù† ÙØ¹Ù„ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡
        
        Returns:
        --------
        str
            Ø²Ù…Ø§Ù† ÙØ¹Ù„ÛŒ
        """
        return datetime.now().strftime("%Y%m%d_%H%M%S")
    
    @staticmethod
    def save_json(data, filepath):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        data : dict
            Ø¯Ø§Ø¯Ù‡
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        """
        ExamUtils.ensure_dir(os.path.dirname(filepath))
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")
    
    @staticmethod
    def load_json(filepath):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ JSON
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        
        Returns:
        --------
        dict
            Ø¯Ø§Ø¯Ù‡
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"ğŸ“‚ JSON Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {filepath}")
        return data
    
    @staticmethod
    def save_pickle(data, filepath):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ pickle
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        data : any
            Ø¯Ø§Ø¯Ù‡
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        """
        ExamUtils.ensure_dir(os.path.dirname(filepath))
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ’¾ Pickle Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")
    
    @staticmethod
    def load_pickle(filepath):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ pickle
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        
        Returns:
        --------
        any
            Ø¯Ø§Ø¯Ù‡
        """
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        print(f"ğŸ“‚ Pickle Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {filepath}")
        return data
    
    @staticmethod
    def save_model(model, filepath, model_type='sklearn'):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        model : any
            Ù…Ø¯Ù„
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„ ('sklearn', 'torch', 'joblib')
        """
        ExamUtils.ensure_dir(os.path.dirname(filepath))
        
        if model_type == 'torch':
            torch.save(model.state_dict(), filepath)
        elif model_type == 'joblib':
            joblib.dump(model, filepath)
        else:
            with open(filepath, 'wb') as f:
                pickle.dump(model, f)
        
        print(f"ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")
    
    @staticmethod
    def load_model(filepath, model_type='sklearn', model_class=None):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„
        model_class : class
            Ú©Ù„Ø§Ø³ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ torch
        
        Returns:
        --------
        any
            Ù…Ø¯Ù„
        """
        if model_type == 'torch':
            if model_class is None:
                raise ValueError("Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ torch Ø¨Ø§ÛŒØ¯ model_class Ù…Ø´Ø®Øµ Ø´ÙˆØ¯")
            model = model_class()
            model.load_state_dict(torch.load(filepath))
        elif model_type == 'joblib':
            model = joblib.load(filepath)
        else:
            with open(filepath, 'rb') as f:
                model = pickle.load(f)
        
        print(f"ğŸ“‚ Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {filepath}")
        return model
    
    @staticmethod
    def get_file_hash(filepath, algorithm='md5'):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ ÙØ§ÛŒÙ„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        algorithm : str
            Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø´
        
        Returns:
        --------
        str
            Ù‡Ø´ ÙØ§ÛŒÙ„
        """
        hash_func = hashlib.new(algorithm)
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_func.update(chunk)
        return hash_func.hexdigest()
    
    @staticmethod
    def get_file_size(filepath):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø­Ø¬Ù… ÙØ§ÛŒÙ„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filepath : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        
        Returns:
        --------
        str
            Ø­Ø¬Ù… ÙØ§ÛŒÙ„
        """
        size = os.path.getsize(filepath)
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} TB"


class MetricsCalculator:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    """
    
    @staticmethod
    def regression_metrics(y_true, y_pred):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        y_true : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ
        y_pred : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        
        Returns:
        --------
        dict
            Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        """
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # Mean Absolute Percentage Error
        mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100
        
        # Explained variance
        explained_var = 1 - np.var(y_true - y_pred) / np.var(y_true)
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'MAPE': mape,
            'Explained Variance': explained_var
        }
    
    @staticmethod
    def classification_metrics(y_true, y_pred, y_pred_proba=None):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        y_true : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        y_pred : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        y_pred_proba : array
            Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        
        Returns:
        --------
        dict
            Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        """
        from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                                   f1_score, confusion_matrix, roc_auc_score)
        
        metrics = {
            'Accuracy': accuracy_score(y_true, y_pred),
            'Precision (macro)': precision_score(y_true, y_pred, average='macro', zero_division=0),
            'Recall (macro)': recall_score(y_true, y_pred, average='macro', zero_division=0),
            'F1 (macro)': f1_score(y_true, y_pred, average='macro', zero_division=0),
            'Precision (weighted)': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'Recall (weighted)': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'F1 (weighted)': f1_score(y_true, y_pred, average='weighted', zero_division=0)
        }
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        metrics['Confusion Matrix'] = cm
        
        # ROC-AUC
        if y_pred_proba is not None:
            n_classes = len(np.unique(y_true))
            if n_classes == 2:
                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba[:, 1])
            else:
                metrics['ROC-AUC (ovr)'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')
                metrics['ROC-AUC (ovo)'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovo')
        
        return metrics


class ExperimentLogger:
    """
    Ú©Ù„Ø§Ø³ Ø«Ø¨Øª Ú¯Ø²Ø§Ø±Ø´ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§
    """
    
    def __init__(self, log_dir='logs'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        log_dir : str
            Ù¾ÙˆØ´Ù‡ Ù„Ø§Ú¯
        """
        self.log_dir = ExamUtils.ensure_dir(log_dir)
        self.log_file = os.path.join(log_dir, f'experiment_{ExamUtils.get_timestamp()}.log')
        self.results = []
        
        print(f"ğŸ“ ExperimentLogger Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {self.log_file}")
    
    def log(self, message, level='INFO'):
        """
        Ø«Ø¨Øª Ù¾ÛŒØ§Ù… Ø¯Ø± Ù„Ø§Ú¯
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        message : str
            Ù¾ÛŒØ§Ù…
        level : str
            Ø³Ø·Ø­ Ù„Ø§Ú¯
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] [{level}] {message}"
        
        print(log_entry)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry + '\n')
    
    def log_config(self, config):
        """
        Ø«Ø¨Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        config : dict
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        """
        self.log("="*60)
        self.log("Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´:")
        for key, value in config.items():
            self.log(f"  {key}: {value}")
        self.log("="*60)
    
    def log_results(self, results, stage):
        """
        Ø«Ø¨Øª Ù†ØªØ§ÛŒØ¬
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        results : dict
            Ù†ØªØ§ÛŒØ¬
        stage : str
            Ù…Ø±Ø­Ù„Ù‡
        """
        self.log(f"\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù…Ø±Ø­Ù„Ù‡ {stage}:")
        for key, value in results.items():
            if isinstance(value, float):
                self.log(f"  {key}: {value:.4f}")
            else:
                self.log(f"  {key}: {value}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        self.results.append({
            'stage': stage,
            'timestamp': datetime.now().isoformat(),
            'results': results
        })
    
    def save_results(self, filename=None):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filename : str
            Ù†Ø§Ù… ÙØ§ÛŒÙ„
        """
        if filename is None:
            filename = f'results_{ExamUtils.get_timestamp()}.json'
        
        filepath = os.path.join(self.log_dir, filename)
        ExamUtils.save_json(self.results, filepath)
        self.log(f"âœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {filepath} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def get_summary(self):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        
        Returns:
        --------
        pd.DataFrame
            Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        """
        summary = []
        for exp in self.results:
            row = {'stage': exp['stage'], 'timestamp': exp['timestamp']}
            row.update(exp['results'])
            summary.append(row)
        
        return pd.DataFrame(summary)


class ConfigManager:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
    """
    
    def __init__(self, config_path='config.json'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        config_path : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        """
        self.config_path = config_path
        self.config = self.load_or_create_config()
    
    def default_config(self):
        """
        Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        
        Returns:
        --------
        dict
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        """
        return {
            'project': {
                'name': 'iran_konkur_project',
                'version': '1.0.0',
                'description': 'Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†'
            },
            'data': {
                'path': 'data/iran_exam.csv',
                'task_type': 'regression',
                'test_size': 0.2,
                'val_size': 0.15,
                'random_state': 42
            },
            'baseline_models': {
                'enabled': True,
                'models': ['Linear', 'Ridge', 'Lasso', 'RF', 'XGB', 'LGBM', 'MLP']
            },
            'tabtransformer': {
                'enabled': True,
                'embedding_dim': 32,
                'num_heads': 4,
                'num_layers': 3,
                'mlp_hidden': [128, 64],
                'dropout': 0.2
            },
            'numerical_embeddings': {
                'enabled': True,
                'methods': ['ple', 'periodic', 'bucket']
            },
            'training': {
                'batch_size': 64,
                'epochs': 100,
                'learning_rate': 0.001,
                'patience': 15
            }
        }
    
    def load_or_create_config(self):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        
        Returns:
        --------
        dict
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        """
        if os.path.exists(self.config_path):
            config = ExamUtils.load_json(self.config_path)
            print(f"ğŸ“‚ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø§Ø² {self.config_path} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        else:
            config = self.default_config()
            ExamUtils.save_json(config, self.config_path)
            print(f"âœ… Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± {self.config_path} Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        
        return config
    
    def get(self, key, default=None):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø¯Ø§Ø± Ø§Ø² Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        key : str
            Ú©Ù„ÛŒØ¯ (Ø¨Ø§ Ù†Ù‚Ø·Ù‡ Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        default : any
            Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        
        Returns:
        --------
        any
            Ù…Ù‚Ø¯Ø§Ø±
        """
        keys = key.split('.')
        value = self.config
        
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
                if value is None:
                    return default
            else:
                return default
        
        return value
    
    def set(self, key, value):
        """
        ØªÙ†Ø¸ÛŒÙ… Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        key : str
            Ú©Ù„ÛŒØ¯
        value : any
            Ù…Ù‚Ø¯Ø§Ø±
        """
        keys = key.split('.')
        config = self.config
        
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        
        config[keys[-1]] = value
        ExamUtils.save_json(self.config, self.config_path)
    
    def update(self, new_config):
        """
        Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        new_config : dict
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¬Ø¯ÛŒØ¯
        """
        self.config.update(new_config)
        ExamUtils.save_json(self.config, self.config_path)


class Timer:
    """
    Ú©Ù„Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø²Ù…Ø§Ù†
    """
    
    def __init__(self, name='Timer'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        name : str
            Ù†Ø§Ù… ØªØ§ÛŒÙ…Ø±
        """
        self.name = name
        self.start_time = None
        self.end_time = None
    
    def __enter__(self):
        self.start()
        return self
    
    def __exit__(self, *args):
        self.stop()
        print(f"â±ï¸ {self.name}: {self.elapsed:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    
    def start(self):
        """Ø´Ø±ÙˆØ¹è®¡æ—¶"""
        self.start_time = time.time()
    
    def stop(self):
        """Ù¾Ø§ÛŒØ§Ù†è®¡æ—¶"""
        self.end_time = time.time()
    
    @property
    def elapsed(self):
        """Ø²Ù…Ø§Ù† Ø³Ù¾Ø±ÛŒ Ø´Ø¯Ù‡"""
        if self.start_time is None:
            return 0
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time
    
    def reset(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ"""
        self.start_time = None
        self.end_time = None


# ============================================
# ØªÙˆØ§Ø¨Ø¹ Ø¢Ù…Ø§Ø±ÛŒ Ú©Ù…Ú©ÛŒ
# ============================================

def calculate_statistics(data):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    data : array
        Ø¯Ø§Ø¯Ù‡
    
    Returns:
    --------
    dict
        Ø¢Ù…Ø§Ø±
    """
    return {
        'count': len(data),
        'mean': np.mean(data),
        'std': np.std(data),
        'min': np.min(data),
        'q25': np.percentile(data, 25),
        'median': np.median(data),
        'q75': np.percentile(data, 75),
        'max': np.max(data),
        'skewness': pd.Series(data).skew(),
        'kurtosis': pd.Series(data).kurtosis()
    }


def normalize_data(X, method='standard'):
    """
    Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    X : array
        Ø¯Ø§Ø¯Ù‡
    method : str
        Ø±ÙˆØ´ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ('standard', 'minmax', 'robust')
    
    Returns:
    --------
    array
        Ø¯Ø§Ø¯Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
    """
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    
    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'robust':
        scaler = RobustScaler()
    else:
        raise ValueError(f"Ø±ÙˆØ´ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {method}")
    
    return scaler.fit_transform(X), scaler


def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):
    """
    ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø³Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    X : array
        ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    y : array
        Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
    train_size : float
        Ù†Ø³Ø¨Øª Ø¢Ù…ÙˆØ²Ø´
    val_size : float
        Ù†Ø³Ø¨Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
    test_size : float
        Ù†Ø³Ø¨Øª Ø¢Ø²Ù…Ø§ÛŒØ´
    random_state : int
        seed
    
    Returns:
    --------
    tuple
        (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    from sklearn.model_selection import train_test_split
    
    # ØªÙ‚Ø³ÛŒÙ… Ø§ÙˆÙ„: Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ù…ÙˆÙ‚Øª
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=(val_size + test_size), random_state=random_state
    )
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯ÙˆÙ…: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´
    val_ratio = val_size / (val_size + test_size)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=test_size/(val_size+test_size), random_state=random_state
    )
    
    return X_train, X_val, X_test, y_train, y_val, y_test


def print_section(title, char='='):
    """
    Ú†Ø§Ù¾ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ø®Ø·
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    title : str
        Ø¹Ù†ÙˆØ§Ù†
    char : str
        Ú©Ø§Ø±Ø§Ú©ØªØ± Ø®Ø·
    """
    print(f"\n{char*60}")
    print(f"{title}")
    print(f"{char*60}")


# ============================================
# ØªØ³Øª
# ============================================

def test_utils():
    """ØªØ³Øª ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ"""
    print("ğŸ§ª ØªØ³Øª ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ")
    print("="*60)
    
    # ØªØ³Øª seed
    ExamUtils.set_seed(42)
    
    # ØªØ³Øª Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡
    ExamUtils.ensure_dir('test_dir')
    
    # ØªØ³Øª Ø²Ù…Ø§Ù†
    with Timer("ØªØ³Øª"):
        import time
        time.sleep(1)
    
    # ØªØ³Øª logger
    logger = ExperimentLogger('test_logs')
    logger.log("Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ù… ØªØ³Øª Ø§Ø³Øª")
    logger.log_config({'test': True, 'value': 123})
    logger.log_results({'rmse': 10.5, 'r2': 0.85}, 'test')
    
    # ØªØ³Øª config
    config_manager = ConfigManager('test_config.json')
    print(f"config.data.path: {config_manager.get('data.path')}")
    
    # ØªØ³Øª metrics
    y_true = np.random.randn(100)
    y_pred = y_true + np.random.randn(100) * 0.1
    
    metrics = MetricsCalculator.regression_metrics(y_true, y_pred)
    print(f"\nğŸ“Š Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    print("\nâœ… Ù‡Ù…Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")


if __name__ == "__main__":
    test_utils()
