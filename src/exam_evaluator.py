"""
Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø´Ø§Ù…Ù„: Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒØŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒØŒ ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§ Ùˆ ...
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,
                           accuracy_score, precision_score, recall_score, f1_score,
                           confusion_matrix, classification_report, roc_auc_score)

from exam_utils import ExamUtils, MetricsCalculator, ExperimentLogger, Timer
from exam_visualization import ExamVisualizer


class ModelEvaluator:
    """
    Ú©Ù„Ø§Ø³ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
    """
    
    def __init__(self, save_dir='evaluation'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        save_dir : str
            Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        """
        self.save_dir = ExamUtils.ensure_dir(save_dir)
        self.results_dir = ExamUtils.ensure_dir(os.path.join(save_dir, 'results'))
        self.plots_dir = ExamUtils.ensure_dir(os.path.join(save_dir, 'plots'))
        
        self.visualizer = ExamVisualizer(self.plots_dir)
        self.logger = ExperimentLogger(os.path.join(save_dir, 'logs'))
        self.results = {}
        
        print(f"ğŸ“Š ModelEvaluator Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: save_dir={save_dir}")
    
    def evaluate_regression(self, y_true, y_pred, model_name='model', 
                            y_train=None, y_pred_train=None):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        y_true : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ
        y_pred : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        y_train : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ Ø¢Ù…ÙˆØ²Ø´
        y_pred_train : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢Ù…ÙˆØ²Ø´
        
        Returns:
        --------
        dict
            Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        """
        print(f"\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†: {model_name}")
        print("="*60)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        metrics = MetricsCalculator.regression_metrics(y_true, y_pred)
        
        # Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        if y_train is not None and y_pred_train is not None:
            train_metrics = MetricsCalculator.regression_metrics(y_train, y_pred_train)
            for key, value in train_metrics.items():
                metrics[f'Train {key}'] = value
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        print("\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        self.visualizer.plot_predictions(y_true, y_pred, 
                                        title=f'{model_name} - Predictions',
                                        filename=f'{model_name}_predictions.jpg')
        
        self.visualizer.plot_residuals(y_true, y_pred,
                                      title=f'{model_name} - Residuals',
                                      filename=f'{model_name}_residuals.jpg')
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        self.results[model_name] = {
            'type': 'regression',
            'metrics': metrics,
            'predictions': {
                'y_true': y_true.tolist() if hasattr(y_true, 'tolist') else y_true,
                'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else y_pred
            }
        }
        
        # Ø«Ø¨Øª Ø¯Ø± Ù„Ø§Ú¯
        self.logger.log_results(metrics, f"{model_name} - Regression")
        
        return metrics
    
    def evaluate_classification(self, y_true, y_pred, y_pred_proba=None,
                               model_name='model', class_names=None,
                               y_train=None, y_pred_train=None):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        y_true : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        y_pred : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        y_pred_proba : array
            Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        class_names : list
            Ù†Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        y_train : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¢Ù…ÙˆØ²Ø´
        y_pred_train : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢Ù…ÙˆØ²Ø´
        
        Returns:
        --------
        dict
            Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        """
        print(f"\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ: {model_name}")
        print("="*60)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        metrics = MetricsCalculator.classification_metrics(y_true, y_pred, y_pred_proba)
        
        # Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        if y_train is not None and y_pred_train is not None:
            train_metrics = MetricsCalculator.classification_metrics(y_train, y_pred_train)
            for key, value in train_metrics.items():
                if isinstance(value, (int, float)):
                    metrics[f'Train {key}'] = value
        
        # Ø­Ø°Ù Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ Ø§Ø² metrics Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
        cm = metrics.pop('Confusion Matrix', None)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        print("\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        # Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ
        if cm is not None:
            self.visualizer.plot_confusion_matrix(
                y_true, y_pred, class_names=class_names,
                title=f'{model_name} - Confusion Matrix',
                filename=f'{model_name}_confusion_matrix.jpg'
            )
        
        # Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÛŒ ROC
        if y_pred_proba is not None:
            n_classes = len(np.unique(y_true))
            self.visualizer.plot_roc_curve(
                y_true, y_pred_proba, n_classes=n_classes,
                class_names=class_names,
                title=f'{model_name} - ROC Curve',
                filename=f'{model_name}_roc_curve.jpg'
            )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        self.results[model_name] = {
            'type': 'classification',
            'metrics': metrics,
            'confusion_matrix': cm.tolist() if cm is not None else None,
            'predictions': {
                'y_true': y_true.tolist() if hasattr(y_true, 'tolist') else y_true,
                'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else y_pred
            }
        }
        
        if y_pred_proba is not None:
            self.results[model_name]['predictions']['y_pred_proba'] = y_pred_proba.tolist()
        
        # Ø«Ø¨Øª Ø¯Ø± Ù„Ø§Ú¯
        self.logger.log_results(metrics, f"{model_name} - Classification")
        
        return metrics
    
    def compare_models(self, results_dict=None, metric='RMSE'):
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        results_dict : dict
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†ØªØ§ÛŒØ¬
        metric : str
            Ù…Ø¹ÛŒØ§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡
        
        Returns:
        --------
        pd.DataFrame
            Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
        """
        if results_dict is None:
            results_dict = self.results
        
        comparison = []
        
        for model_name, result in results_dict.items():
            row = {'Model': model_name}
            
            if result['type'] == 'regression':
                metrics = result['metrics']
                row['RMSE'] = metrics.get('RMSE', 0)
                row['MAE'] = metrics.get('MAE', 0)
                row['R2'] = metrics.get('R2', 0)
                row['MAPE'] = metrics.get('MAPE', 0)
            else:
                metrics = result['metrics']
                row['Accuracy'] = metrics.get('Accuracy', 0)
                row['F1 (macro)'] = metrics.get('F1 (macro)', 0)
                row['ROC-AUC'] = metrics.get('ROC-AUC', metrics.get('ROC-AUC (ovr)', 0))
            
            comparison.append(row)
        
        df_comparison = pd.DataFrame(comparison)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        if result['type'] == 'regression':
            df_comparison = df_comparison.sort_values('RMSE')
        else:
            df_comparison = df_comparison.sort_values('Accuracy', ascending=False)
        
        # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡
        self.visualizer.plot_model_comparison(
            df_comparison, metric=metric,
            title='Model Comparison',
            filename='model_comparison.jpg'
        )
        
        # Ø°Ø®ÛŒØ±Ù‡
        comparison_path = os.path.join(self.results_dir, 'model_comparison.csv')
        df_comparison.to_csv(comparison_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø± {comparison_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return df_comparison
    
    def error_analysis(self, y_true, y_pred, model_name='model', 
                      bins=10, save=True):
        """
        ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        y_true : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ
        y_pred : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        bins : int
            ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§
        save : bool
            Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        
        Returns:
        --------
        dict
            ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§
        """
        errors = y_true - y_pred
        abs_errors = np.abs(errors)
        
        # Ø¢Ù…Ø§Ø± Ø®Ø·Ø§Ù‡Ø§
        error_stats = {
            'mean_error': np.mean(errors),
            'std_error': np.std(errors),
            'mean_abs_error': np.mean(abs_errors),
            'median_abs_error': np.median(abs_errors),
            'max_error': np.max(abs_errors),
            'min_error': np.min(abs_errors),
            'q25_error': np.percentile(abs_errors, 25),
            'q75_error': np.percentile(abs_errors, 75)
        }
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ
        bins_labels = pd.qcut(y_true, q=bins, labels=False, duplicates='drop')
        error_by_bin = {}
        
        for i in range(len(np.unique(bins_labels))):
            mask = bins_labels == i
            if np.sum(mask) > 0:
                bin_true = y_true[mask]
                bin_pred = y_pred[mask]
                bin_errors = errors[mask]
                
                error_by_bin[f'Bin_{i+1}'] = {
                    'range': f"{bin_true.min():.0f}-{bin_true.max():.0f}",
                    'count': np.sum(mask),
                    'mean_error': np.mean(bin_errors),
                    'std_error': np.std(bin_errors),
                    'mae': np.mean(np.abs(bin_errors))
                }
        
        # Ø±Ø³Ù… ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§
        if save:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. ØªÙˆØ²ÛŒØ¹ Ø®Ø·Ø§Ù‡Ø§
            axes[0, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)
            axes[0, 0].axvline(x=0, color='r', linestyle='--', lw=2)
            axes[0, 0].set_xlabel('Error')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].set_title('Error Distribution')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. Ø®Ø·Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ
            axes[0, 1].scatter(y_true, errors, alpha=0.5, s=10)
            axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)
            axes[0, 1].set_xlabel('Actual Values')
            axes[0, 1].set_ylabel('Error')
            axes[0, 1].set_title('Error vs Actual')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. Ø®Ø·Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
            axes[1, 0].scatter(y_pred, errors, alpha=0.5, s=10)
            axes[1, 0].axhline(y=0, color='r', linestyle='--', lw=2)
            axes[1, 0].set_xlabel('Predicted Values')
            axes[1, 0].set_ylabel('Error')
            axes[1, 0].set_title('Error vs Predicted')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. Ø®Ø·Ø§ÛŒ Ù…Ø·Ù„Ù‚ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡
            bins_list = list(error_by_bin.keys())
            mae_values = [error_by_bin[b]['mae'] for b in bins_list]
            
            axes[1, 1].bar(range(len(bins_list)), mae_values, color='skyblue', edgecolor='black')
            axes[1, 1].set_xlabel('Value Range')
            axes[1, 1].set_ylabel('MAE')
            axes[1, 1].set_title('MAE by Value Range')
            axes[1, 1].set_xticks(range(len(bins_list)))
            axes[1, 1].set_xticklabels([error_by_bin[b]['range'] for b in bins_list], 
                                       rotation=45, ha='right')
            axes[1, 1].grid(True, alpha=0.3, axis='y')
            
            plt.suptitle(f'Error Analysis - {model_name}', fontsize=14, y=1.02)
            plt.tight_layout()
            
            save_path = os.path.join(self.plots_dir, f'{model_name}_error_analysis.jpg')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
        
        return {
            'error_stats': error_stats,
            'error_by_bin': error_by_bin
        }
    
    def cross_validation_report(self, cv_scores, model_name='model'):
        """
        Ú¯Ø²Ø§Ø±Ø´ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        cv_scores : list
            Ù„ÛŒØ³Øª Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        
        Returns:
        --------
        dict
            Ú¯Ø²Ø§Ø±Ø´
        """
        report = {
            'mean_score': np.mean(cv_scores),
            'std_score': np.std(cv_scores),
            'min_score': np.min(cv_scores),
            'max_score': np.max(cv_scores),
            'scores': cv_scores
        }
        
        # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±
        plt.figure(figsize=(10, 6))
        
        plt.plot(range(1, len(cv_scores) + 1), cv_scores, 'o-', color='steelblue', linewidth=2)
        plt.axhline(y=report['mean_score'], color='r', linestyle='--', 
                   label=f"Mean: {report['mean_score']:.4f}")
        plt.fill_between(range(1, len(cv_scores) + 1),
                        report['mean_score'] - report['std_score'],
                        report['mean_score'] + report['std_score'],
                        alpha=0.2, color='gray', label=f"Std: {report['std_score']:.4f}")
        
        plt.xlabel('Fold')
        plt.ylabel('Score')
        plt.title(f'Cross-Validation Scores - {model_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        save_path = os.path.join(self.plots_dir, f'{model_name}_cv_scores.jpg')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return report
    
    def generate_full_report(self, filename='evaluation_report.txt'):
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filename : str
            Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ú¯Ø²Ø§Ø±Ø´
        """
        report_path = os.path.join(self.results_dir, filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n")
            f.write("="*80 + "\n\n")
            f.write(f"ØªØ§Ø±ÛŒØ®: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡: {len(self.results)}\n\n")
            
            for model_name, result in self.results.items():
                f.write("-"*60 + "\n")
                f.write(f"ğŸ“Œ Ù…Ø¯Ù„: {model_name}\n")
                f.write(f"Ù†ÙˆØ¹: {result['type']}\n")
                f.write("\nÙ…Ø¹ÛŒØ§Ø±Ù‡Ø§:\n")
                
                for key, value in result['metrics'].items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                
                f.write("\n")
            
            f.write("="*80 + "\n")
        
        print(f"ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø± {report_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def save_results(self, filename='evaluation_results.json'):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        filename : str
            Ù†Ø§Ù… ÙØ§ÛŒÙ„
        """
        filepath = os.path.join(self.results_dir, filename)
        ExamUtils.save_json(self.results, filepath)
        print(f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {filepath} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")


class EnsembleEvaluator:
    """
    Ú©Ù„Ø§Ø³ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ensemble
    """
    
    def __init__(self, evaluator):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        evaluator : ModelEvaluator
            Ø´ÛŒØ¡ Ø§Ø±Ø²ÛŒØ§Ø¨
        """
        self.evaluator = evaluator
    
    def evaluate_voting_ensemble(self, predictions_dict, y_true, weights=None):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ensemble Ø¨Ø§ voting
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        predictions_dict : dict
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
        y_true : array
            Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ
        weights : list
            ÙˆØ²Ù†â€ŒÙ‡Ø§
        
        Returns:
        --------
        dict
            Ù†ØªØ§ÛŒØ¬
        """
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
        all_preds = np.array(list(predictions_dict.values()))
        
        if weights is not None:
            weights = np.array(weights) / np.sum(weights)
            ensemble_pred = np.average(all_preds, axis=0, weights=weights)
        else:
            ensemble_pred = np.mean(all_preds, axis=0)
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        metrics = self.evaluator.evaluate_regression(
            y_true, ensemble_pred, model_name='Voting Ensemble'
        )
        
        return metrics
    
    def evaluate_stacking_ensemble(self, base_models, meta_model, 
                                   X_train, y_train, X_test, y_test):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ stacking ensemble
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        base_models : list
            Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        meta_model : any
            Ù…Ø¯Ù„ ÙØ±Ø§
        X_train, y_train : array
            Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´
        X_test, y_test : array
            Ø¯Ø§Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´
        
        Returns:
        --------
        dict
            Ù†ØªØ§ÛŒØ¬
        """
        from sklearn.ensemble import StackingRegressor
        
        # Ø§ÛŒØ¬Ø§Ø¯ stacking ensemble
        estimators = [(f'model_{i}', model) for i, model in enumerate(base_models)]
        stack = StackingRegressor(estimators=estimators, final_estimator=meta_model)
        
        # Ø¢Ù…ÙˆØ²Ø´
        stack.fit(X_train, y_train)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        y_pred = stack.predict(X_test)
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        metrics = self.evaluator.evaluate_regression(
            y_test, y_pred, model_name='Stacking Ensemble'
        )
        
        return metrics, stack


# ============================================
# ØªØ³Øª
# ============================================

def test_evaluator():
    """ØªØ³Øª Ú©Ù„Ø§Ø³ Ø§Ø±Ø²ÛŒØ§Ø¨"""
    print("ğŸ§ª ØªØ³Øª ModelEvaluator")
    print("="*60)
    
    # Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    np.random.seed(42)
    y_true = np.random.randn(1000)
    y_pred1 = y_true + np.random.randn(1000) * 0.1
    y_pred2 = y_true + np.random.randn(1000) * 0.2
    y_pred3 = y_true + np.random.randn(1000) * 0.15
    
    # Ø§ÛŒØ¬Ø§Ø¯ evaluator
    evaluator = ModelEvaluator('test_eval')
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
    evaluator.evaluate_regression(y_true, y_pred1, model_name='Model_1')
    evaluator.evaluate_regression(y_true, y_pred2, model_name='Model_2')
    evaluator.evaluate_regression(y_true, y_pred3, model_name='Model_3')
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡
    comparison = evaluator.compare_models()
    print("\nğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
    print(comparison)
    
    # ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§
    evaluator.error_analysis(y_true, y_pred1, model_name='Model_1')
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    evaluator.save_results()
    evaluator.generate_full_report()
    
    print("\nâœ… Ù‡Ù…Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")


if __name__ == "__main__":
    test_evaluator()
