"""
Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
Ø´Ø§Ù…Ù„ Û³ Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ: MLPØŒ Random Forest Ùˆ Gradient Boosting
"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')


class BaselineModels:
    """
    Ú©Ù„Ø§Ø³ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    Ø´Ø§Ù…Ù„ Û³ Ù…Ø¯Ù„: MLPØŒ Random ForestØŒ Gradient Boosting
    """
    
    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, task_type='regression'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        X_train : array
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        y_train : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        X_val : array
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        y_val : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        X_test : array
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´
        y_test : array
            Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´
        task_type : str
            Ù†ÙˆØ¹ ÙˆØ¸ÛŒÙÙ‡ (ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· regression)
        """
        self.X_train = X_train
        self.y_train = y_train
        self.X_val = X_val
        self.y_val = y_val
        self.X_test = X_test
        self.y_test = y_test
        self.task_type = task_type
        
        self.models = {}
        self.results = []
        self.predictions = {}
        self.training_times = {}
        
        print(f"ğŸ“Š BaselineModels Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        print(f"   Ø¢Ù…ÙˆØ²Ø´: {X_train.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
        print(f"   Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {X_val.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
        print(f"   Ø¢Ø²Ù…Ø§ÛŒØ´: {X_test.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
    
    def define_models(self):
        """
        ØªØ¹Ø±ÛŒÙ Û³ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
        """
        print("\nğŸ“‹ ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡...")
        
        self.models = {
            'MLP': MLPRegressor(
                hidden_layer_sizes=(64, 32),
                activation='relu',
                solver='adam',
                max_iter=500,
                random_state=42,
                early_stopping=True,
                validation_fraction=0.1,
                n_iter_no_change=10
            ),
            
            'Random Forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=5,
                min_samples_leaf=2,
                subsample=0.8,
                random_state=42
            )
        }
        
        print(f"âœ… {len(self.models)} Ù…Ø¯Ù„ ØªØ¹Ø±ÛŒÙ Ø´Ø¯:")
        for name in self.models.keys():
            print(f"   - {name}")
    
    def train_and_evaluate(self, verbose=True):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        verbose : bool
            Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¢Ù…ÙˆØ²Ø´
        
        Returns:
        --------
        pd.DataFrame
            Ù†ØªØ§ÛŒØ¬ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        """
        print("\n" + "="*60)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡")
        print("="*60)
        
        for name, model in self.models.items():
            if verbose:
                print(f"\nğŸ“ˆ Ø¢Ù…ÙˆØ²Ø´ {name}...")
            
            start_time = time.time()
            
            try:
                # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
                model.fit(self.X_train, self.y_train)
                
                # Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´
                training_time = time.time() - start_time
                self.training_times[name] = training_time
                
                # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
                y_pred_train = model.predict(self.X_train)
                y_pred_val = model.predict(self.X_val)
                y_pred_test = model.predict(self.X_test)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
                result = {
                    'Model': name,
                    'Train RMSE': np.sqrt(mean_squared_error(self.y_train, y_pred_train)),
                    'Val RMSE': np.sqrt(mean_squared_error(self.y_val, y_pred_val)),
                    'Test RMSE': np.sqrt(mean_squared_error(self.y_test, y_pred_test)),
                    'Train MAE': mean_absolute_error(self.y_train, y_pred_train),
                    'Val MAE': mean_absolute_error(self.y_val, y_pred_val),
                    'Test MAE': mean_absolute_error(self.y_test, y_pred_test),
                    'Train R2': r2_score(self.y_train, y_pred_train),
                    'Val R2': r2_score(self.y_val, y_pred_val),
                    'Test R2': r2_score(self.y_test, y_pred_test),
                    'Time (s)': training_time
                }
                
                self.results.append(result)
                self.predictions[name] = y_pred_test
                
                if verbose:
                    print(f"   âœ… Test RMSE: {result['Test RMSE']:.2f}, RÂ²: {result['Test R2']:.4f}, Ø²Ù…Ø§Ù†: {training_time:.2f}s")
                
            except Exception as e:
                if verbose:
                    print(f"   âŒ Ø®Ø·Ø§: {e}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ù†ØªØ§ÛŒØ¬
        results_df = pd.DataFrame(self.results)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ RMSE
        results_df = results_df.sort_values('Test RMSE')
        
        print("\n" + "="*60)
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        print("="*60)
        
        return results_df
    
    def get_best_model(self, metric='Test RMSE'):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø± Ù…Ø´Ø®Øµ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        metric : str
            Ù…Ø¹ÛŒØ§Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ('Test RMSE', 'Test R2')
        
        Returns:
        --------
        tuple
            (Ù†Ø§Ù… Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„, Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø±)
        """
        if not self.results:
            return None, None
        
        df = pd.DataFrame(self.results)
        
        if metric == 'Test R2':
            best_idx = df[metric].argmax()
            best_value = df[metric].max()
        else:
            best_idx = df[metric].argmin()
            best_value = df[metric].min()
        
        best_model = df.iloc[best_idx]['Model']
        
        return best_model, best_value
    
    def plot_comparison(self, save_path='plots/baseline_comparison.jpg'):
        """
        Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        save_path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±
        """
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        colors = ['#3498db', '#2ecc71', '#e74c3c']
        
        # 1. RMSE Comparison
        axes[0, 0].bar(df['Model'], df['Test RMSE'], color=colors, edgecolor='black')
        axes[0, 0].set_xlabel('Model')
        axes[0, 0].set_ylabel('RMSE')
        axes[0, 0].set_title('Test RMSE Comparison (lower is better)')
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # 2. MAE Comparison
        axes[0, 1].bar(df['Model'], df['Test MAE'], color=colors, edgecolor='black')
        axes[0, 1].set_xlabel('Model')
        axes[0, 1].set_ylabel('MAE')
        axes[0, 1].set_title('Test MAE Comparison (lower is better)')
        axes[0, 1].grid(True, alpha=0.3, axis='y')
        
        # 3. RÂ² Comparison
        axes[0, 2].bar(df['Model'], df['Test R2'], color=colors, edgecolor='black')
        axes[0, 2].set_xlabel('Model')
        axes[0, 2].set_ylabel('RÂ²')
        axes[0, 2].set_title('Test RÂ² Comparison (higher is better)')
        axes[0, 2].grid(True, alpha=0.3, axis='y')
        axes[0, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        # 4. Training Time
        axes[1, 0].bar(df['Model'], df['Time (s)'], color=colors, edgecolor='black')
        axes[1, 0].set_xlabel('Model')
        axes[1, 0].set_ylabel('Time (seconds)')
        axes[1, 0].set_title('Training Time Comparison')
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # 5. Train vs Test RMSE
        x = np.arange(len(df))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, df['Train RMSE'], width, label='Train', color='skyblue', edgecolor='black')
        axes[1, 1].bar(x + width/2, df['Test RMSE'], width, label='Test', color='salmon', edgecolor='black')
        axes[1, 1].set_xlabel('Model')
        axes[1, 1].set_ylabel('RMSE')
        axes[1, 1].set_title('Train vs Test RMSE')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(df['Model'])
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        # 6. Model Performance Summary
        axes[1, 2].axis('off')
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø®Ù„Ø§ØµÙ‡
        best_model, best_rmse = self.get_best_model('Test RMSE')
        best_r2_model, best_r2 = self.get_best_model('Test R2')
        
        summary_text = f"ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:\n\n"
        summary_text += f"Ø¨Ù‡ØªØ±ÛŒÙ† RMSE: {best_model}\n"
        summary_text += f"   RMSE = {best_rmse:.2f}\n\n"
        summary_text += f"Ø¨Ù‡ØªØ±ÛŒÙ† RÂ²: {best_r2_model}\n"
        summary_text += f"   RÂ² = {best_r2:.4f}\n\n"
        
        for _, row in df.iterrows():
            summary_text += f"{row['Model']}:\n"
            summary_text += f"   RMSE={row['Test RMSE']:.2f}, RÂ²={row['Test R2']:.3f}\n"
        
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,
                       fontsize=10, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.suptitle('Baseline Models Comparison', fontsize=16, y=1.02)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def plot_predictions(self, model_name, save_path=None):
        """
        Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø¯Ù„ Ø®Ø§Øµ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        model_name : str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        save_path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±
        """
        if model_name not in self.predictions:
            print(f"âŒ Ù…Ø¯Ù„ {model_name} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return
        
        y_pred = self.predictions[model_name]
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Scatter plot
        axes[0].scatter(self.y_test, y_pred, alpha=0.5, s=10)
        axes[0].plot([self.y_test.min(), self.y_test.max()], 
                    [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0].set_xlabel('Actual Values')
        axes[0].set_ylabel('Predicted Values')
        axes[0].set_title(f'{model_name}: Actual vs Predicted')
        axes[0].grid(True, alpha=0.3)
        
        # Residuals
        residuals = self.y_test - y_pred
        axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
        axes[1].set_xlabel('Residuals')
        axes[1].set_ylabel('Frequency')
        axes[1].set_title(f'{model_name}: Residual Distribution')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def generate_report(self, save_path='reports/baseline_report.txt'):
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø§Ø² Ù†ØªØ§ÛŒØ¬
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        save_path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        """
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results)
        best_model, best_rmse = self.get_best_model('Test RMSE')
        best_r2_model, best_r2 = self.get_best_model('Test R2')
        
        report = []
        report.append("="*70)
        report.append("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡")
        report.append("="*70)
        report.append(f"ØªØ§Ø±ÛŒØ®: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡
        report.append("ğŸ“‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡:")
        report.append(f"   ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {len(self.X_train)}")
        report.append(f"   ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(self.X_val)}")
        report.append(f"   ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´: {len(self.X_test)}")
        report.append(f"   ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {self.X_train.shape[1]}")
        report.append("")
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§
        report.append("ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
        report.append(f"   Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ RMSE: {best_model} (RMSE={best_rmse:.2f})")
        report.append(f"   Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ RÂ²: {best_r2_model} (RÂ²={best_r2:.4f})")
        report.append("")
        
        # Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§ÛŒØ¬
        report.append("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„:")
        report.append("-" * 80)
        for _, row in df.iterrows():
            report.append(f"   {row['Model']:15s} | RMSE={row['Test RMSE']:8.2f} | RÂ²={row['Test R2']:.4f} | MAE={row['Test MAE']:7.2f} | Ø²Ù…Ø§Ù†={row['Time (s)']:.2f}s")
        
        report.append("")
        report.append("="*70)
        report.append("âœ… Ù¾Ø§ÛŒØ§Ù† Ú¯Ø²Ø§Ø±Ø´")
        report.append("="*70)
        
        report_text = "\n".join(report)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return report_text
    
    def save_results(self, path='results/baseline_results.csv'):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ CSV
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡
        """
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results)
        df = df.sort_values('Test RMSE')
        
        import os
        os.makedirs(os.path.dirname(path), exist_ok=True)
        df.to_csv(path, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return df


# ============================================
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÛŒØ¹
# ============================================

def run_baseline_quick(X_train, y_train, X_test, y_test):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÛŒØ¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    X_train, y_train : array
        Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´
    X_test, y_test : array
        Ø¯Ø§Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´
    
    Returns:
    --------
    tuple
        (Ù†ØªØ§ÛŒØ¬, Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„, Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²)
    """
    baseline = BaselineModels(X_train, y_train, X_test, y_test, X_test, y_test)
    baseline.define_models()
    results = baseline.train_and_evaluate(verbose=False)
    best_model, best_score = baseline.get_best_model()
    
    return results, best_model, best_score


if __name__ == "__main__":
    # ØªØ³Øª Ø³Ø±ÛŒØ¹
    print("ğŸ§ª ØªØ³Øª Ú©Ù„Ø§Ø³ BaselineModels")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    from sklearn.datasets import make_regression
    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
    
    # Ø§Ø¬Ø±Ø§
    baseline = BaselineModels(X_train, y_train, X_val, y_val, X_test, y_test)
    baseline.define_models()
    results = baseline.train_and_evaluate()
    baseline.plot_comparison()
    baseline.generate_report()
