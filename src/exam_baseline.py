"""
Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Û³ Ù…Ø¯Ù„: MLPØŒ Random Forest Ùˆ Gradient Boosting
"""

import os  # â— Ø§ÛŒÙ† Ø®Ø· Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')


class BaselineModels:
    """
    Ú©Ù„Ø§Ø³ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    Ø´Ø§Ù…Ù„ Û³ Ù…Ø¯Ù„: MLPØŒ Random ForestØŒ Gradient Boosting
    """
    
    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, task_type='regression'):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù‡
        """
        self.X_train = X_train
        self.y_train = y_train
        self.X_val = X_val
        self.y_val = y_val
        self.X_test = X_test
        self.y_test = y_test
        self.task_type = task_type
        
        self.models = {}
        self.results = []
        self.predictions = {}
        self.training_times = {}
        
        print(f"ğŸ“Š BaselineModels Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        print(f"   Ø¢Ù…ÙˆØ²Ø´: {X_train.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
        print(f"   Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {X_val.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
        print(f"   Ø¢Ø²Ù…Ø§ÛŒØ´: {X_test.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
    
    def define_models(self):
        """ØªØ¹Ø±ÛŒÙ Û³ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡"""
        print("\nğŸ“‹ ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡...")
        
        self.models = {
            'MLP': MLPRegressor(
                hidden_layer_sizes=(64, 32), 
                max_iter=500, 
                random_state=42
            ),
            'Random Forest': RandomForestRegressor(
                n_estimators=100, 
                random_state=42, 
                n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=100, 
                learning_rate=0.1, 
                random_state=42
            )
        }
        
        print(f"âœ… {len(self.models)} Ù…Ø¯Ù„ ØªØ¹Ø±ÛŒÙ Ø´Ø¯")
        for i, (name, _) in enumerate(self.models.items(), 1):
            print(f"   {i:2d}. {name}")
    
    def train_and_evaluate(self, verbose=True):
        """Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        print("\n" + "="*80)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡")
        print("="*80)
        
        for name, model in self.models.items():
            if verbose:
                print(f"\nğŸ“ˆ Ø¢Ù…ÙˆØ²Ø´ {name}...")
            
            start_time = time.time()
            
            try:
                # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
                model.fit(self.X_train, self.y_train)
                
                # Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´
                training_time = time.time() - start_time
                self.training_times[name] = training_time
                
                # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
                y_pred_train = model.predict(self.X_train)
                y_pred_val = model.predict(self.X_val)
                y_pred_test = model.predict(self.X_test)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
                result = {
                    'Model': name,
                    'Train RMSE': np.sqrt(mean_squared_error(self.y_train, y_pred_train)),
                    'Val RMSE': np.sqrt(mean_squared_error(self.y_val, y_pred_val)),
                    'Test RMSE': np.sqrt(mean_squared_error(self.y_test, y_pred_test)),
                    'Train MAE': mean_absolute_error(self.y_train, y_pred_train),
                    'Val MAE': mean_absolute_error(self.y_val, y_pred_val),
                    'Test MAE': mean_absolute_error(self.y_test, y_pred_test),
                    'Train R2': r2_score(self.y_train, y_pred_train),
                    'Val R2': r2_score(self.y_val, y_pred_val),
                    'Test R2': r2_score(self.y_test, y_pred_test),
                    'Time (s)': training_time
                }
                
                self.results.append(result)
                self.predictions[name] = y_pred_test
                
                if verbose:
                    print(f"   âœ… Test RMSE: {result['Test RMSE']:.2f}, RÂ²: {result['Test R2']:.4f}, Ø²Ù…Ø§Ù†: {training_time:.2f}s")
                
            except Exception as e:
                if verbose:
                    print(f"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ {name}: {e}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ù†ØªØ§ÛŒØ¬
        results_df = pd.DataFrame(self.results)
        if not results_df.empty:
            results_df = results_df.sort_values('Test RMSE')
        
        print("\n" + "="*80)
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        print("="*80)
        
        return results_df
    
    def get_best_model(self, metric='Test RMSE'):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø± Ù…Ø´Ø®Øµ"""
        if not self.results:
            return None, None
        
        df = pd.DataFrame(self.results)
        if metric == 'Test R2':
            best_idx = df[metric].argmax()
            best_value = df[metric].max()
        else:
            best_idx = df[metric].argmin()
            best_value = df[metric].min()
        
        return df.iloc[best_idx]['Model'], best_value
    
    def plot_comparison(self, save_path='plots/baseline_comparison.jpg'):
        """Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. RMSE Ù…Ù‚Ø§ÛŒØ³Ù‡
        axes[0, 0].barh(df['Model'], df['Test RMSE'], 
                       color='skyblue', edgecolor='black')
        axes[0, 0].set_xlabel('RMSE (lower is better)')
        axes[0, 0].set_title('RMSE Comparison')
        axes[0, 0].grid(True, alpha=0.3, axis='x')
        
        # 2. RÂ² Ù…Ù‚Ø§ÛŒØ³Ù‡
        axes[0, 1].barh(df['Model'], df['Test R2'], 
                       color='lightgreen', edgecolor='black')
        axes[0, 1].set_xlabel('RÂ² (higher is better)')
        axes[0, 1].set_title('RÂ² Comparison')
        axes[0, 1].grid(True, alpha=0.3, axis='x')
        axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.5)
        
        # 3. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´
        axes[1, 0].barh(df['Model'], df['Time (s)'], 
                       color='salmon', edgecolor='black')
        axes[1, 0].set_xlabel('Training Time (seconds)')
        axes[1, 0].set_title('Training Time Comparison')
        axes[1, 0].grid(True, alpha=0.3, axis='x')
        
        # 4. Train vs Test RMSE
        x = np.arange(len(df))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, df['Train RMSE'], width, 
                      label='Train', color='skyblue', edgecolor='black')
        axes[1, 1].bar(x + width/2, df['Test RMSE'], width,
                      label='Test', color='lightcoral', edgecolor='black')
        axes[1, 1].set_xlabel('Model')
        axes[1, 1].set_ylabel('RMSE')
        axes[1, 1].set_title('Train vs Test RMSE')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(df['Model'], rotation=45, ha='right')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle('Baseline Models Comparison', fontsize=16, y=1.02)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def generate_report(self, save_path='reports/baseline_report.txt'):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø§Ø² Ù†ØªØ§ÛŒØ¬"""
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results)
        best_model, best_rmse = self.get_best_model('Test RMSE')
        
        report = []
        report.append("="*80)
        report.append("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù…Ø±Ø­Ù„Ù‡ Û±: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡")
        report.append("="*80)
        report.append(f"ØªØ§Ø±ÛŒØ®: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        report.append("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
        
        for _, row in df.iterrows():
            report.append(f"\n{row['Model']}:")
            report.append(f"   Test RMSE: {row['Test RMSE']:.2f}")
            report.append(f"   Test RÂ²: {row['Test R2']:.4f}")
            report.append(f"   Test MAE: {row['Test MAE']:.2f}")
            report.append(f"   Ø²Ù…Ø§Ù†: {row['Time (s)']:.2f}s")
        
        report.append("")
        report.append("-"*80)
        report.append(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model} Ø¨Ø§ RMSE={best_rmse:.2f}")
        report.append("")
        report.append("="*80)
        
        report_text = "\n".join(report)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ reports Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return report_text
    
    def save_results(self, path='results/baseline_results.csv'):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ CSV"""
        if not self.results:
            print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        df = pd.DataFrame(self.results).sort_values('Test RMSE')
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ results Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        df.to_csv(path, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return df
