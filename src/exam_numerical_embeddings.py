"""
Ø¬Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer
Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø§Ù„Ù‡: On Embeddings for Numerical Features in Tabular Deep Learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


class PiecewiseLinearEncoding(nn.Module):
    """
    Piecewise Linear Encoding (PLE)
    ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø¨Ù‡ ØªØ±Ú©ÛŒØ¨ Ø®Ø·ÛŒ Ù‚Ø·Ø¹Ù‡â€ŒØ§ÛŒ
    """
    def __init__(self, num_features, num_bins=10, embedding_dim=32, temperature=0.1):
        super(PiecewiseLinearEncoding, self).__init__()
        
        self.num_features = num_features
        self.num_bins = num_bins
        self.embedding_dim = embedding_dim
        self.temperature = temperature
        
        # Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª Ù‚Ø§Ø¨Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.breakpoints = nn.Parameter(torch.linspace(0, 1, num_bins + 1))
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‚Ø·Ø¹Ù‡
        self.linear_weights = nn.Parameter(
            torch.randn(num_features, num_bins, embedding_dim) * 0.01
        )
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ [0, 1]
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ ØªØ§ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø´Ú©Ø³Øª
        breakpoints = self.breakpoints.view(1, 1, -1)
        distances = torch.abs(x_norm.unsqueeze(-1) - breakpoints)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ membership Ø¯Ø± Ù‡Ø± Ù‚Ø·Ø¹Ù‡ Ø¨Ø§ softmax
        memberships = F.softmax(-distances[..., :-1] / self.temperature, dim=-1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding
        embedding = torch.einsum('bfk, fkd -> bfd', memberships, self.linear_weights)
        
        return embedding


class PeriodicEncoding(nn.Module):
    """
    Periodic Activations
    Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ sin Ùˆ cos
    """
    def __init__(self, num_features, embedding_dim=32, num_frequencies=8):
        super(PeriodicEncoding, self).__init__()
        
        self.num_features = num_features
        self.embedding_dim = embedding_dim
        self.num_frequencies = num_frequencies
        
        # ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.frequencies = nn.Parameter(
            torch.exp(torch.linspace(math.log(1), math.log(10), num_frequencies))
        )
        
        # Ù„Ø§ÛŒÙ‡ ØªØ±Ú©ÛŒØ¨
        self.combine = nn.Linear(num_features * num_frequencies * 2, embedding_dim)
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, 1, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # Ú¯Ø³ØªØ±Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        x_expanded = x_norm.unsqueeze(-1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ sin Ùˆ cos
        freq = self.frequencies.view(1, 1, -1)
        
        sin_comp = torch.sin(2 * np.pi * freq * x_expanded)
        cos_comp = torch.cos(2 * np.pi * freq * x_expanded)
        
        # ØªØ±Ú©ÛŒØ¨
        periodic_features = torch.stack([sin_comp, cos_comp], dim=-1)
        periodic_flat = periodic_features.reshape(batch_size, -1)
        
        # projection
        embedding = self.combine(periodic_flat)
        
        return embedding.unsqueeze(1)


class BucketEmbedding(nn.Module):
    """
    Bucket Embedding
    ØªÙ‚Ø³ÛŒÙ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø¨Ù‡ Ø³Ø·Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø²Ø§
    """
    def __init__(self, num_features, num_buckets=20, embedding_dim=32):
        super(BucketEmbedding, self).__init__()
        
        self.num_features = num_features
        self.num_buckets = num_buckets
        self.embedding_dim = embedding_dim
        
        # Ù…Ø±Ø²Ù‡Ø§ÛŒ Ø³Ø·Ù„â€ŒÙ‡Ø§
        self.register_buffer('boundaries', torch.linspace(0, 1, num_buckets - 1))
        
        # embedding Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ
        self.embeddings = nn.ModuleList([
            nn.Embedding(num_buckets, embedding_dim) for _ in range(num_features)
        ])
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ù„
        boundaries = torch.cat([
            torch.tensor([0.0]).to(x.device),
            self.boundaries,
            torch.tensor([1.0]).to(x.device)
        ])
        
        bucket_indices = torch.bucketize(x_norm, boundaries) - 1
        bucket_indices = torch.clamp(bucket_indices, 0, self.num_buckets - 1)
        
        # Ø¯Ø±ÛŒØ§ÙØª embedding
        embeddings = []
        for i in range(num_features):
            emb = self.embeddings[i](bucket_indices[:, i])
            embeddings.append(emb)
        
        return torch.stack(embeddings, dim=1)


class TabTransformerWithNumEmbedding(nn.Module):
    """
    TabTransformer Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
    """
    def __init__(self, num_categorical, num_continuous, categories,
                 num_embedding_type='ple',
                 embedding_dim=32, num_heads=4, num_layers=3,
                 mlp_hidden_dims=[128, 64],
                 transformer_dropout=0.1, mlp_dropout=0.2,
                 output_dim=1):
        super(TabTransformerWithNumEmbedding, self).__init__()
        
        self.num_categorical = num_categorical
        self.num_continuous = num_continuous
        self.num_embedding_type = num_embedding_type
        self.embedding_dim = embedding_dim
        
        # Embedding Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.cat_embeddings = nn.ModuleList([
            nn.Embedding(cat, embedding_dim) for cat in categories
        ])
        
        # Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if num_continuous > 0:
            if num_embedding_type == 'ple':
                self.num_embedding = PiecewiseLinearEncoding(
                    num_continuous, num_bins=10, embedding_dim=embedding_dim
                )
            elif num_embedding_type == 'periodic':
                self.num_embedding = PeriodicEncoding(
                    num_continuous, embedding_dim=embedding_dim
                )
            elif num_embedding_type == 'bucket':
                self.num_embedding = BucketEmbedding(
                    num_continuous, num_buckets=20, embedding_dim=embedding_dim
                )
            else:
                self.num_proj = nn.Linear(num_continuous, embedding_dim)
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=embedding_dim * 4,
            dropout=transformer_dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Transformer
        # Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ ÛŒÚ© embedding Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ embedding_dim Ø¯Ø§Ø±Ø¯
        # Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ø¯Ø¯ÛŒ ÛŒÚ© embedding Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ embedding_dim Ø¯Ø§Ø±Ø¯ (Ø¨Ø¹Ø¯ Ø§Ø² Ø¬Ø§Ø³Ø§Ø²ÛŒ)
        transformer_output_dim = embedding_dim * (num_categorical + num_continuous)
        print(f"ðŸ“ Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Transformer: {transformer_output_dim}")
        
        # MLP Ù†Ù‡Ø§ÛŒÛŒ
        mlp_layers = []
        prev_dim = transformer_output_dim
        
        for i, hidden_dim in enumerate(mlp_hidden_dims):
            mlp_layers.append(nn.Linear(prev_dim, hidden_dim))
            mlp_layers.append(nn.ReLU())
            mlp_layers.append(nn.Dropout(mlp_dropout))
            prev_dim = hidden_dim
            print(f"   Ù„Ø§ÛŒÙ‡ {i+1}: {prev_dim} -> {hidden_dim}")
        
        mlp_layers.append(nn.Linear(prev_dim, output_dim))
        print(f"   Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ: {prev_dim} -> {output_dim}")
        
        self.mlp = nn.Sequential(*mlp_layers)
        
        print(f"âœ… Ù…Ø¯Ù„ TabTransformerWithNumEmbedding Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
        print(f"   ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {num_categorical}, ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {num_continuous}")
        print(f"   Ø§Ø¨Ø¹Ø§Ø¯ embedding: {embedding_dim}")
        print(f"   Ø¨Ø¹Ø¯ Ú©Ù„: {transformer_output_dim}")
    
    def forward(self, x_cat, x_cont):
        batch_size = x_cat.shape[0]
        
        # Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        cat_embedded = []
        for i in range(self.num_categorical):
            emb = self.cat_embeddings[i](x_cat[:, i])
            cat_embedded.append(emb)
        
        cat_embedded = torch.stack(cat_embedded, dim=1)  # (batch, num_cat, emb_dim)
        
        # Ø¬Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if self.num_continuous > 0:
            if hasattr(self, 'num_embedding'):
                # Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
                num_embedded = self.num_embedding(x_cont)  # (batch, num_cont, emb_dim)
            else:
                # projection Ø³Ø§Ø¯Ù‡
                num_embedded = self.num_proj(x_cont).unsqueeze(1)  # (batch, 1, emb_dim)
                # Ø§Ú¯Ø± num_continuous > 1ØŒ Ø¨Ø§ÛŒØ¯ ØªÚ©Ø±Ø§Ø± Ø´ÙˆØ¯
                if self.num_continuous > 1:
                    num_embedded = num_embedded.repeat(1, self.num_continuous, 1)
        else:
            num_embedded = torch.empty(batch_size, 0, self.embedding_dim).to(x_cat.device)
        
        # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ embeddings
        all_embeddings = torch.cat([cat_embedded, num_embedded], dim=1)
        # all_embeddings shape: (batch, num_cat + num_cont, emb_dim)
        
        # Ø¹Ø¨ÙˆØ± Ø§Ø² Transformer
        transformed = self.transformer(all_embeddings)
        # transformed shape: (batch, num_cat + num_cont, emb_dim)
        
        # Flatten Ú©Ø±Ø¯Ù†
        flattened = transformed.reshape(batch_size, -1)
        # flattened shape: (batch, (num_cat + num_cont) * emb_dim)
        
        # MLP Ù†Ù‡Ø§ÛŒÛŒ
        output = self.mlp(flattened)
        
        return output.squeeze() if output.shape[1] == 1 else output
