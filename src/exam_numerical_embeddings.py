"""
Ø¬Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ TabTransformer
Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø§Ù„Ù‡: On Embeddings for Numerical Features in Tabular Deep Learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


# ============================================
# Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
# ============================================

class PiecewiseLinearEncoding(nn.Module):
    """
    Piecewise Linear Encoding (PLE)
    ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø¨Ù‡ ØªØ±Ú©ÛŒØ¨ Ø®Ø·ÛŒ Ù‚Ø·Ø¹Ù‡â€ŒØ§ÛŒ
    """
    def __init__(self, num_features, num_bins=10, embedding_dim=32, 
                 temperature=0.1, learnable_breaks=True):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        num_features : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        num_bins : int
            ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø·Ø¹Ø§Øª
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        temperature : float
            Ø¯Ù…Ø§ Ø¨Ø±Ø§ÛŒ softmax
        learnable_breaks : bool
            ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª
        """
        super(PiecewiseLinearEncoding, self).__init__()
        
        self.num_features = num_features
        self.num_bins = num_bins
        self.embedding_dim = embedding_dim
        self.temperature = temperature
        
        # Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª (Ø¨ÛŒÙ† 0 Ùˆ 1)
        if learnable_breaks:
            # Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª Ù‚Ø§Ø¨Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
            self.breakpoints = nn.Parameter(torch.linspace(0, 1, num_bins + 1))
        else:
            # Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª Ø«Ø§Ø¨Øª
            self.register_buffer('breakpoints', torch.linspace(0, 1, num_bins + 1))
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‚Ø·Ø¹Ù‡
        self.linear_weights = nn.Parameter(
            torch.randn(num_features, num_bins, embedding_dim) * 0.01
        )
        self.linear_biases = nn.Parameter(
            torch.zeros(num_features, num_bins, embedding_dim)
        )
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ [0, 1]
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ ØªØ§ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø´Ú©Ø³Øª
        # Ø´Ú©Ù„: (batch_size, num_features, num_bins + 1)
        breakpoints = self.breakpoints.view(1, 1, -1)
        distances = torch.abs(x_norm.unsqueeze(-1) - breakpoints)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ membership Ø¯Ø± Ù‡Ø± Ù‚Ø·Ø¹Ù‡ Ø¨Ø§ softmax
        # Ø´Ú©Ù„: (batch_size, num_features, num_bins)
        memberships = F.softmax(-distances[..., :-1] / self.temperature, dim=-1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding
        # memberships: (b, f, bins) @ weights: (f, bins, d) -> (b, f, d)
        embedding = torch.einsum('bfk, fkd -> bfd', memberships, self.linear_weights)
        
        return embedding


class PeriodicEncoding(nn.Module):
    """
    Periodic Activations
    Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ sin Ùˆ cos
    """
    def __init__(self, num_features, embedding_dim=32, num_frequencies=8,
                 min_freq=1, max_freq=10, trainable=True):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        num_features : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        num_frequencies : int
            ØªØ¹Ø¯Ø§Ø¯ ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§
        min_freq : float
            Ø­Ø¯Ø§Ù‚Ù„ ÙØ±Ú©Ø§Ù†Ø³
        max_freq : float
            Ø­Ø¯Ø§Ú©Ø«Ø± ÙØ±Ú©Ø§Ù†Ø³
        trainable : bool
            ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§
        """
        super(PeriodicEncoding, self).__init__()
        
        self.num_features = num_features
        self.embedding_dim = embedding_dim
        self.num_frequencies = num_frequencies
        
        if trainable:
            # ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
            self.frequencies = nn.Parameter(
                torch.exp(torch.linspace(math.log(min_freq), math.log(max_freq), num_frequencies))
            )
            self.phases = nn.Parameter(
                torch.randn(num_features, num_frequencies) * 0.1
            )
        else:
            # ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª
            self.register_buffer(
                'frequencies',
                torch.exp(torch.linspace(math.log(min_freq), math.log(max_freq), num_frequencies))
            )
            self.register_buffer(
                'phases',
                torch.zeros(num_features, num_frequencies)
            )
        
        # Ù„Ø§ÛŒÙ‡ ØªØ±Ú©ÛŒØ¨
        self.combine = nn.Linear(num_features * num_frequencies * 2, embedding_dim)
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, 1, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        x_min = x.min(dim=0, keepdim=True)[0]
        x_max = x.max(dim=0, keepdim=True)[0]
        x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # Ú¯Ø³ØªØ±Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        x_expanded = x_norm.unsqueeze(-1)  # (b, f, 1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ sin Ùˆ cos
        # freq: (num_freq) -> (1, 1, num_freq)
        freq = self.frequencies.view(1, 1, -1)
        # phase: (f, num_freq) -> (1, f, num_freq)
        phase = self.phases.unsqueeze(0)
        
        sin_comp = torch.sin(2 * np.pi * freq * x_expanded + phase)
        cos_comp = torch.cos(2 * np.pi * freq * x_expanded + phase)
        
        # ØªØ±Ú©ÛŒØ¨
        # (b, f, num_freq, 2) -> (b, f * num_freq * 2)
        periodic_features = torch.stack([sin_comp, cos_comp], dim=-1)
        periodic_flat = periodic_features.reshape(batch_size, -1)
        
        # projection
        embedding = self.combine(periodic_flat)
        
        return embedding.unsqueeze(1)  # (b, 1, d)


class BucketEmbedding(nn.Module):
    """
    Bucket Embedding
    ØªÙ‚Ø³ÛŒÙ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø¨Ù‡ Ø³Ø·Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø²Ø§
    """
    def __init__(self, num_features, num_buckets=20, embedding_dim=32,
                 strategy='linear', learnable=True):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        num_features : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        num_buckets : int
            ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ù„â€ŒÙ‡Ø§
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        strategy : str
            Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ØªÙ‚Ø³ÛŒÙ…: 'linear', 'quantile', 'log'
        learnable : bool
            ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø±Ø² Ø³Ø·Ù„â€ŒÙ‡Ø§
        """
        super(BucketEmbedding, self).__init__()
        
        self.num_features = num_features
        self.num_buckets = num_buckets
        self.embedding_dim = embedding_dim
        self.strategy = strategy
        
        if learnable:
            # Ù…Ø±Ø²Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
            self.boundaries = nn.Parameter(torch.linspace(0, 1, num_buckets - 1))
        else:
            # Ù…Ø±Ø²Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª
            self.register_buffer('boundaries', torch.linspace(0, 1, num_buckets - 1))
        
        # embedding Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ
        self.embeddings = nn.ModuleList([
            nn.Embedding(num_buckets, embedding_dim) for _ in range(num_features)
        ])
    
    def forward(self, x):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features)
        
        Returns:
        --------
        torch.Tensor
            embedding Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_features, embedding_dim)
        """
        batch_size, num_features = x.shape
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        if self.strategy == 'quantile':
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² quantile (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´)
            # Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø² min-max Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            x_min = x.min(dim=0, keepdim=True)[0]
            x_max = x.max(dim=0, keepdim=True)[0]
            x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        elif self.strategy == 'log':
            # ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ
            x_norm = torch.log1p(x - x.min())
            x_norm = x_norm / x_norm.max()
        else:  # linear
            x_min = x.min(dim=0, keepdim=True)[0]
            x_max = x.max(dim=0, keepdim=True)[0]
            x_norm = (x - x_min) / (x_max - x_min + 1e-8)
        
        # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ù„
        boundaries = torch.sigmoid(self.boundaries)
        boundaries = torch.cat([
            torch.tensor([0.0]).to(x.device),
            boundaries,
            torch.tensor([1.0]).to(x.device)
        ])
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ù†Ø¯ÛŒØ³ Ø³Ø·Ù„
        # bucketize Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        bucket_indices = torch.bucketize(x_norm, boundaries) - 1
        bucket_indices = torch.clamp(bucket_indices, 0, self.num_buckets - 1)
        
        # Ø¯Ø±ÛŒØ§ÙØª embedding
        embeddings = []
        for i in range(num_features):
            emb = self.embeddings[i](bucket_indices[:, i])
            embeddings.append(emb)
        
        return torch.stack(embeddings, dim=1)  # (b, f, d)


# ============================================
# Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ TabTransformer Ø¨Ø§ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
# ============================================

class TabTransformerWithNumEmbedding(nn.Module):
    """
    TabTransformer Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
    ØªØ±Ú©ÛŒØ¨ Ù…Ù‚Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ TabTransformer Ùˆ Numerical Embeddings
    """
    
    def __init__(self, num_categorical, num_continuous, categories,
                 num_embedding_type='ple',  # 'ple', 'periodic', 'bucket', 'none'
                 embedding_dim=32, num_heads=4, num_layers=3,
                 mlp_hidden_dims=[128, 64], mlp_dropout=0.2,
                 transformer_dropout=0.1, output_dim=1):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        num_categorical : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        num_continuous : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        categories : list
            ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        num_embedding_type : str
            Ù†ÙˆØ¹ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ: 'ple', 'periodic', 'bucket', 'none'
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        num_heads : int
            ØªØ¹Ø¯Ø§Ø¯ headÙ‡Ø§ÛŒ attention
        num_layers : int
            ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        mlp_hidden_dims : list
            Ø§Ø¨Ø¹Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† MLP
        mlp_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± MLP
        transformer_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± Transformer
        output_dim : int
            Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ
        """
        super(TabTransformerWithNumEmbedding, self).__init__()
        
        self.num_categorical = num_categorical
        self.num_continuous = num_continuous
        self.num_embedding_type = num_embedding_type
        
        # 1. Embedding Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.cat_embeddings = nn.ModuleList([
            nn.Embedding(cat, embedding_dim) for cat in categories
        ])
        
        # 2. Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if num_continuous > 0:
            if num_embedding_type == 'ple':
                self.num_embedding = PiecewiseLinearEncoding(
                    num_continuous, num_bins=10, embedding_dim=embedding_dim
                )
            elif num_embedding_type == 'periodic':
                self.num_embedding = PeriodicEncoding(
                    num_continuous, embedding_dim=embedding_dim
                )
            elif num_embedding_type == 'bucket':
                self.num_embedding = BucketEmbedding(
                    num_continuous, num_buckets=20, embedding_dim=embedding_dim
                )
            else:  # 'none' - projection Ø³Ø§Ø¯Ù‡
                self.num_projection = nn.Linear(num_continuous, embedding_dim)
        
        # 3. Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=embedding_dim * 4,
            dropout=transformer_dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # 4. MLP Ù†Ù‡Ø§ÛŒÛŒ
        total_embeddings = num_categorical + num_continuous
        mlp_input_dim = embedding_dim * total_embeddings
        
        mlp_layers = []
        prev_dim = mlp_input_dim
        
        for hidden_dim in mlp_hidden_dims:
            mlp_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(mlp_dropout)
            ])
            prev_dim = hidden_dim
        
        mlp_layers.append(nn.Linear(prev_dim, output_dim))
        
        self.mlp = nn.Sequential(*mlp_layers)
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        self._init_weights()
    
    def _init_weights(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x_cat, x_cont):
        """
        Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        -----------
        x_cat : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        x_cont : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        """
        batch_size = x_cat.shape[0]
        
        # 1. Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        cat_embedded = []
        for i in range(self.num_categorical):
            emb = self.cat_embeddings[i](x_cat[:, i])
            cat_embedded.append(emb)
        
        cat_embedded = torch.stack(cat_embedded, dim=1)  # (b, num_cat, d)
        
        # 2. Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if self.num_continuous > 0:
            if hasattr(self, 'num_embedding'):
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ø§Ø³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
                num_embedded = self.num_embedding(x_cont)
            else:
                # projection Ø³Ø§Ø¯Ù‡
                num_embedded = self.num_projection(x_cont).unsqueeze(1)
        else:
            num_embedded = torch.empty(batch_size, 0, cat_embedded.size(2)).to(x_cat.device)
        
        # 3. ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ embeddings
        all_embeddings = torch.cat([cat_embedded, num_embedded], dim=1)
        
        # 4. Ø¹Ø¨ÙˆØ± Ø§Ø² Transformer
        transformed = self.transformer(all_embeddings)
        flattened = transformed.reshape(batch_size, -1)
        
        # 5. MLP Ù†Ù‡Ø§ÛŒÛŒ
        output = self.mlp(flattened)
        
        return output.squeeze() if output.shape[1] == 1 else output


# ============================================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# ============================================

def create_numerical_embedding_layer(num_features, method='ple', **kwargs):
    """
    Ø§ÛŒØ¬Ø§Ø¯ Ù„Ø§ÛŒÙ‡ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    -----------
    num_features : int
        ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    method : str
        Ø±ÙˆØ´ Ø¬Ø§Ø³Ø§Ø²ÛŒ
    **kwargs : dict
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    
    Returns:
    --------
    nn.Module
        Ù„Ø§ÛŒÙ‡ Ø¬Ø§Ø³Ø§Ø²ÛŒ
    """
    if method == 'ple':
        return PiecewiseLinearEncoding(num_features, **kwargs)
    elif method == 'periodic':
        return PeriodicEncoding(num_features, **kwargs)
    elif method == 'bucket':
        return BucketEmbedding(num_features, **kwargs)
    else:
        return nn.Linear(num_features, kwargs.get('embedding_dim', 32))


def test_numerical_embeddings():
    """ØªØ³Øª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ"""
    print("ğŸ§ª ØªØ³Øª Numerical Embeddings")
    print("="*60)
    
    batch_size = 32
    num_features = 5
    x = torch.randn(batch_size, num_features)
    
    methods = ['ple', 'periodic', 'bucket']
    
    for method in methods:
        print(f"\nğŸ“Œ ØªØ³Øª {method}:")
        
        if method == 'ple':
            embed_layer = PiecewiseLinearEncoding(num_features, num_bins=10, embedding_dim=16)
        elif method == 'periodic':
            embed_layer = PeriodicEncoding(num_features, embedding_dim=16)
        else:  # bucket
            embed_layer = BucketEmbedding(num_features, num_buckets=20, embedding_dim=16)
        
        output = embed_layer(x)
        print(f"   ÙˆØ±ÙˆØ¯ÛŒ shape: {x.shape}")
        print(f"   Ø®Ø±ÙˆØ¬ÛŒ shape: {output.shape}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {sum(p.numel() for p in embed_layer.parameters()):,}")
    
    print("\nâœ… Ù‡Ù…Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")


if __name__ == "__main__":
    test_numerical_embeddings()
