"""
Ù…Ø±Ø­Ù„Ù‡ Û²: Ù…Ø¯Ù„ TabTransformer Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
===================================================
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ø´Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ TabTransformer Ø§Ø³Øª.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ reproducibility
torch.manual_seed(42)
np.random.seed(42)

# ==================================================
# Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ
# ==================================================

class DataManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±"""
    
    def __init__(self, data_path=None):
        self.data_path = data_path or Path('../data/konkur_data.csv')
        self.data = None
        self.categorical_cols = None
        self.numerical_cols = None
        self.target_col = 'rank'
        
    def load_and_preprocess(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        print("ğŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
        self.data = pd.read_csv(self.data_path)
        print(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯: {self.data.shape}")
        
        # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
        cols_to_drop = ['Unnamed: 0', 'id']  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
        existing_cols = [col for col in cols_to_drop if col in self.data.columns]
        if existing_cols:
            self.data = self.data.drop(columns=existing_cols)
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.numerical_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()
        if self.target_col in self.numerical_cols:
            self.numerical_cols.remove(self.target_col)
            
        self.categorical_cols = self.data.select_dtypes(include=['object']).columns.tolist()
        
        print(f"ğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(self.numerical_cols)}")
        print(f"ğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(self.categorical_cols)}")
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self._clean_data()
        
        return self.data
    
    def _clean_data(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        initial_len = len(self.data)
        self.data = self.data.dropna()
        print(f"ğŸ§¹ {initial_len - len(self.data)} Ø±Ø¯ÛŒÙ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯")
        
        # Ø­Ø°Ù outliers Ø¯Ø± Ø±ØªØ¨Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ø±ØªØ¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒ 20000)
        if self.target_col in self.data.columns:
            outliers = len(self.data[self.data[self.target_col] > 20000])
            self.data = self.data[self.data[self.target_col] <= 20000]
            print(f"ğŸ§¹ {outliers} Ø±Ø¯ÛŒÙ outlier Ø­Ø°Ù Ø´Ø¯Ù†Ø¯")
    
    def prepare_data(self, cat_embed_dims=None):
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ TabTransformer"""
        if cat_embed_dims is None:
            cat_embed_dims = {}
            
        # Ø§Ù†Ú©Ø¯ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.label_encoders = {}
        for col in self.categorical_cols:
            if col in self.data.columns:
                self.label_encoders[col] = LabelEncoder()
                self.data[col] = self.label_encoders[col].fit_transform(self.data[col].astype(str))
        
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        self.scaler = StandardScaler()
        if self.numerical_cols:
            self.data[self.numerical_cols] = self.scaler.fit_transform(self.data[self.numerical_cols])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ embedding Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.cat_embed_dims = {}
        for col in self.categorical_cols:
            if col in self.data.columns:
                n_categories = len(self.data[col].unique())
                # ÙØ±Ù…ÙˆÙ„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ: min(50, (n_categories + 1) // 2)
                embed_dim = cat_embed_dims.get(col, min(50, (n_categories + 1) // 2))
                self.cat_embed_dims[col] = (n_categories, embed_dim)
        
        return self.data


# ==================================================
# Ù…Ø¹Ù…Ø§Ø±ÛŒ TabTransformer
# ==================================================

class TransformerBlock(nn.Module):
    """Ø¨Ù„ÙˆÚ© ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Self-attention with residual
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feed-forward with residual
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        
        return x


class TabTransformer(nn.Module):
    """
    Ù…Ø¯Ù„ TabTransformer Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ÛŒ
    ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² embedding Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
    """
    
    def __init__(self, 
                 cat_embed_dims,      # Ù„ÛŒØ³Øª (ØªØ¹Ø¯Ø§Ø¯_Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§, Ø¨Ø¹Ø¯_embedding) Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
                 numerical_dim,        # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                 transformer_dim=64,   # Ø¨Ø¹Ø¯ Ø®Ø±ÙˆØ¬ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
                 n_heads=8,            # ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±Ù‡Ø§ÛŒ attention
                 n_layers=6,           # ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
                 ff_dim=128,            # Ø¨Ø¹Ø¯ Ù„Ø§ÛŒÙ‡ feed-forward
                 dropout=0.1,
                 task='regression'):   # regression ÛŒØ§ classification
        super().__init__()
        
        self.task = task
        self.numerical_dim = numerical_dim
        self.transformer_dim = transformer_dim
        
        # Embedding Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.cat_embeddings = nn.ModuleList()
        for n_cat, embed_dim in cat_embed_dims:
            self.cat_embeddings.append(
                nn.Embedding(n_cat, embed_dim)
            )
        
        # Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† embeddingâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ù‡ ÙØ¶Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
        total_cat_dim = sum(embed_dim for _, embed_dim in cat_embed_dims)
        self.cat_proj = nn.Linear(total_cat_dim, transformer_dim) if cat_embed_dims else None
        
        # Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if numerical_dim > 0:
            self.num_proj = nn.Linear(numerical_dim, transformer_dim)
        
        # Positional encoding (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ)
        self.pos_encoding = nn.Parameter(torch.randn(1, max(1, len(cat_embed_dims)), transformer_dim))
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(transformer_dim, n_heads, ff_dim, dropout)
            for _ in range(n_layers)
        ])
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        self.output_layer = nn.Sequential(
            nn.LayerNorm(transformer_dim),
            nn.Linear(transformer_dim, transformer_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(transformer_dim // 2, 1 if task == 'regression' else 2)  # Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ
        )
        
    def forward(self, categorical, numerical=None):
        """
        Args:
            categorical: tensor Ø¨Ø§ shape (batch_size, n_cat_features)
            numerical: tensor Ø¨Ø§ shape (batch_size, n_num_features) ÛŒØ§ None
        """
        batch_size = categorical.shape[0]
        
        # Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        cat_embeds = []
        for i, emb_layer in enumerate(self.cat_embeddings):
            cat_embeds.append(emb_layer(categorical[:, i]))
        
        if cat_embeds:
            # Concatenate Ù‡Ù…Ù‡ embeddingâ€ŒÙ‡Ø§
            cat_combined = torch.cat(cat_embeds, dim=1)  # (batch_size, total_cat_dim)
            
            # Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† Ø¨Ù‡ ÙØ¶Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
            cat_features = self.cat_proj(cat_combined).unsqueeze(1)  # (batch_size, 1, transformer_dim)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† positional encoding
            cat_features = cat_features + self.pos_encoding[:, :1, :]
            
            x = cat_features
        else:
            x = None
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if numerical is not None and self.numerical_dim > 0:
            num_features = self.num_proj(numerical).unsqueeze(1)  # (batch_size, 1, transformer_dim)
            if x is None:
                x = num_features
            else:
                x = torch.cat([x, num_features], dim=1)  # (batch_size, n_tokens, transformer_dim)
        
        # Ø¹Ø¨ÙˆØ± Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
        for transformer in self.transformer_blocks:
            x = transformer(x)
        
        # Ú¯Ø±ÙØªÙ† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒ ØªÙ…Ø§Ù… tokenâ€ŒÙ‡Ø§
        x = x.mean(dim=1)  # (batch_size, transformer_dim)
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        output = self.output_layer(x)
        
        if self.task == 'regression':
            return output.squeeze(-1)
        else:
            return output


# ==================================================
# Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
# ==================================================

def create_data_loaders(X_cat, X_num, y, train_idx, val_idx, test_idx, batch_size=128):
    """Ø§ÛŒØ¬Ø§Ø¯ DataLoader Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´"""
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ tensor
    X_cat_tensor = torch.tensor(X_cat, dtype=torch.long)
    X_num_tensor = torch.tensor(X_num, dtype=torch.float32) if X_num is not None else None
    y_tensor = torch.tensor(y, dtype=torch.float32)
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    train_cat = X_cat_tensor[train_idx]
    train_num = X_num_tensor[train_idx] if X_num is not None else None
    train_y = y_tensor[train_idx]
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
    val_cat = X_cat_tensor[val_idx]
    val_num = X_num_tensor[val_idx] if X_num is not None else None
    val_y = y_tensor[val_idx]
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´
    test_cat = X_cat_tensor[test_idx]
    test_num = X_num_tensor[test_idx] if X_num is not None else None
    test_y = y_tensor[test_idx]
    
    # Ø§ÛŒØ¬Ø§Ø¯ Dataset
    if X_num is not None:
        train_dataset = TensorDataset(train_cat, train_num, train_y)
        val_dataset = TensorDataset(val_cat, val_num, val_y)
        test_dataset = TensorDataset(test_cat, test_num, test_y)
    else:
        train_dataset = TensorDataset(train_cat, train_y)
        val_dataset = TensorDataset(val_cat, val_y)
        test_dataset = TensorDataset(test_cat, test_y)
    
    # Ø§ÛŒØ¬Ø§Ø¯ DataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, test_loader


def train_epoch(model, train_loader, optimizer, criterion, device):
    """ÛŒÚ© epoch Ø¢Ù…ÙˆØ²Ø´"""
    model.train()
    total_loss = 0
    
    for batch in train_loader:
        if len(batch) == 3:
            cat, num, y = batch
            cat, num, y = cat.to(device), num.to(device), y.to(device)
        else:
            cat, y = batch
            cat, y = cat.to(device), y.to(device)
            num = None
        
        optimizer.zero_grad()
        output = model(cat, num)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)


def validate(model, val_loader, criterion, device):
    """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ"""
    model.eval()
    total_loss = 0
    predictions = []
    targets = []
    
    with torch.no_grad():
        for batch in val_loader:
            if len(batch) == 3:
                cat, num, y = batch
                cat, num, y = cat.to(device), num.to(device), y.to(device)
            else:
                cat, y = batch
                cat, y = cat.to(device), y.to(device)
                num = None
            
            output = model(cat, num)
            loss = criterion(output, y)
            
            total_loss += loss.item()
            predictions.extend(output.cpu().numpy())
            targets.extend(y.cpu().numpy())
    
    predictions = np.array(predictions)
    targets = np.array(targets)
    
    rmse = np.sqrt(mean_squared_error(targets, predictions))
    r2 = r2_score(targets, predictions)
    
    return total_loss / len(val_loader), rmse, r2


def train_tabtransformer(model, train_loader, val_loader, epochs=50, lr=1e-3, device='cuda', patience=10):
    """Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„ TabTransformer"""
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    criterion = nn.MSELoss()
    
    best_val_loss = float('inf')
    patience_counter = 0
    history = {'train_loss': [], 'val_loss': [], 'val_rmse': [], 'val_r2': []}
    
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ TabTransformer...")
    
    for epoch in range(epochs):
        # Ø¢Ù…ÙˆØ²Ø´
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        val_loss, val_rmse, val_r2 = validate(model, val_loader, criterion, device)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ scheduler
        scheduler.step(val_loss)
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_rmse'].append(val_rmse)
        history['val_r2'].append(val_r2)
        
        # Ú†Ø§Ù¾ Ù¾ÛŒØ´Ø±ÙØª
        if (epoch + 1) % 5 == 0:
            print(f"  Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | RMSE: {val_rmse:.2f} | RÂ²: {val_r2:.4f}")
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
            torch.save(model.state_dict(), 'best_tabtransformer.pt')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  â¹ï¸ Early stopping Ø¯Ø± epoch {epoch+1}")
                break
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
    model.load_state_dict(torch.load('best_tabtransformer.pt'))
    
    return model, history


# ==================================================
# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ù…Ø±Ø­Ù„Ù‡ Û²
# ==================================================

def run_stage2():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±Ø­Ù„Ù‡ Û²"""
    
    print("\n" + "="*70)
    print("ğŸ¯ Ù…Ø±Ø­Ù„Ù‡ Û²: Ù…Ø¯Ù„ TabTransformer")
    print("="*70 + "\n")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
    Path("results/stage2").mkdir(parents=True, exist_ok=True)
    Path("plots/stage2").mkdir(parents=True, exist_ok=True)
    
    # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    print("ğŸ“ Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
    print("-" * 50)
    
    data_manager = DataManager()
    data = data_manager.load_and_preprocess()
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ TabTransformer
    cat_embed_dims_config = {}  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ù„Ø®ÙˆØ§Ù‡ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯
    data = data_manager.prepare_data(cat_embed_dims_config)
    
    print(f"\nâœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯:")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(data)}")
    print(f"   - ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(data_manager.numerical_cols)}")
    print(f"   - ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(data_manager.categorical_cols)}")
    
    # 2. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‡Ø¯Ù
    print("\n" + "ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ Û²: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
    print("-" * 50)
    
    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
    if data_manager.categorical_cols:
        X_cat = data[data_manager.categorical_cols].values.astype(np.int64)
        cat_embed_dims = [(len(data[col].unique()), 
                          data_manager.cat_embed_dims[col][1]) 
                         for col in data_manager.categorical_cols]
    else:
        X_cat = np.zeros((len(data), 0))
        cat_embed_dims = []
    
    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
    if data_manager.numerical_cols:
        X_num = data[data_manager.numerical_cols].values.astype(np.float32)
        numerical_dim = len(data_manager.numerical_cols)
    else:
        X_num = None
        numerical_dim = 0
    
    # Ù‡Ø¯Ù
    y = data[data_manager.target_col].values.astype(np.float32)
    
    print(f"   X_cat shape: {X_cat.shape}")
    print(f"   X_num shape: {X_num.shape if X_num is not None else 'None'}")
    print(f"   y shape: {y.shape}")
    
    # 3. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    print("\n" + "âœ‚ï¸ Ù…Ø±Ø­Ù„Ù‡ Û³: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
    print("-" * 50)
    
    # ØªÙ‚Ø³ÛŒÙ… 70-15-15
    n = len(data)
    train_idx = np.random.choice(n, int(0.7 * n), replace=False)
    remaining = np.setdiff1d(np.arange(n), train_idx)
    val_idx = np.random.choice(remaining, int(0.5 * len(remaining)), replace=False)
    test_idx = np.setdiff1d(remaining, val_idx)
    
    print(f"   Train: {len(train_idx)} samples")
    print(f"   Val: {len(val_idx)} samples")
    print(f"   Test: {len(test_idx)} samples")
    
    # 4. Ø§ÛŒØ¬Ø§Ø¯ DataLoader
    print("\n" + "ğŸ”„ Ù…Ø±Ø­Ù„Ù‡ Û´: Ø§ÛŒØ¬Ø§Ø¯ DataLoader")
    print("-" * 50)
    
    train_loader, val_loader, test_loader = create_data_loaders(
        X_cat, X_num, y, train_idx, val_idx, test_idx, batch_size=128
    )
    
    print("âœ… DataLoaderÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯")
    
    # 5. Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
    print("\n" + "ğŸ—ï¸ Ù…Ø±Ø­Ù„Ù‡ Ûµ: Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ TabTransformer")
    print("-" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   Ø¯Ø³ØªÚ¯Ø§Ù‡: {device}")
    
    model = TabTransformer(
        cat_embed_dims=cat_embed_dims,
        numerical_dim=numerical_dim,
        transformer_dim=64,
        n_heads=8,
        n_layers=4,
        ff_dim=128,
        dropout=0.1,
        task='regression'
    ).to(device)
    
    print(f"\nğŸ“‹ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„:")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±: 4")
    print(f"   - Ø¨Ø¹Ø¯ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±: 64")
    print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±Ù‡Ø§ÛŒ attention: 8")
    
    # 6. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
    print("\n" + "ğŸš€ Ù…Ø±Ø­Ù„Ù‡ Û¶: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„")
    print("-" * 50)
    
    start_time = time.time()
    model, history = train_tabtransformer(
        model, train_loader, val_loader,
        epochs=50, lr=1e-3, device=device, patience=10
    )
    training_time = time.time() - start_time
    
    print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ø²Ù…Ø§Ù†: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    
    # 7. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ Û·: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ")
    print("-" * 50)
    
    criterion = nn.MSELoss()
    test_loss, test_rmse, test_r2 = validate(model, test_loader, criterion, device)
    
    print(f"\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´:")
    print(f"   RMSE: {test_rmse:.2f}")
    print(f"   RÂ²: {test_r2:.4f}")
    
    # 8. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    print("\n" + "ğŸ“ˆ Ù…Ø±Ø­Ù„Ù‡ Û¸: Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§")
    print("-" * 50)
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # Loss
    axes[0].plot(history['train_loss'], label='Train Loss', alpha=0.8)
    axes[0].plot(history['val_loss'], label='Val Loss', alpha=0.8)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # RMSE
    axes[1].plot(history['val_rmse'], color='orange', alpha=0.8)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('RMSE')
    axes[1].set_title('Validation RMSE')
    axes[1].grid(True, alpha=0.3)
    
    # RÂ²
    axes[2].plot(history['val_r2'], color='green', alpha=0.8)
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('RÂ²')
    axes[2].set_title('Validation RÂ²')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('plots/stage2/training_history.jpg', dpi=100, bbox_inches='tight')
    plt.show()
    print("ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± plots/stage2/training_history.jpg Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # 9. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    print("\n" + "ğŸ’¾ Ù…Ø±Ø­Ù„Ù‡ Û¹: Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬")
    print("-" * 50)
    
    results = {
        'stage': 2,
        'model': 'TabTransformer',
        'timestamp': str(datetime.now()),
        'data_info': {
            'n_samples': len(data),
            'n_categorical': len(data_manager.categorical_cols),
            'n_numerical': len(data_manager.numerical_cols)
        },
        'model_params': {
            'transformer_dim': 64,
            'n_heads': 8,
            'n_layers': 4,
            'ff_dim': 128,
            'dropout': 0.1,
            'total_params': sum(p.numel() for p in model.parameters())
        },
        'training': {
            'epochs': len(history['train_loss']),
            'final_train_loss': history['train_loss'][-1],
            'final_val_loss': history['val_loss'][-1],
            'best_val_rmse': min(history['val_rmse']),
            'best_val_r2': max(history['val_r2']),
            'training_time_seconds': training_time
        },
        'test_results': {
            'rmse': float(test_rmse),
            'r2': float(test_r2),
            'loss': float(test_loss)
        }
    }
    
    # Ø°Ø®ÛŒØ±Ù‡ JSON
    with open('results/stage2/tabtransformer_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Ø°Ø®ÛŒØ±Ù‡ CSV Ù†ØªØ§ÛŒØ¬
    results_df = pd.DataFrame({
        'metric': ['RMSE', 'RÂ²', 'Train Loss', 'Val Loss', 'Training Time (s)'],
        'value': [test_rmse, test_r2, history['train_loss'][-1], history['val_loss'][-1], training_time]
    })
    results_df.to_csv('results/stage2/tabtransformer_results.csv', index=False)
    
    print("âœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± results/stage2/ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    
    # 10. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "="*70)
    print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø±Ø­Ù„Ù‡ Û²")
    print("="*70)
    print(f"ØªØ§Ø±ÛŒØ®: {datetime.now()}")
    print("\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ TabTransformer:")
    print(f"   RMSE: {test_rmse:.2f}")
    print(f"   RÂ²: {test_r2:.4f}")
    print(f"   Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    print("\nâœ… Ù…Ø±Ø­Ù„Ù‡ Û² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
    print("="*70 + "\n")
    
    return model, history, results


# ==================================================
# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
# ==================================================

if __name__ == "__main__":
    run_stage2()
